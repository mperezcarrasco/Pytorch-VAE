{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "from preprocess import get_mnist, get_webcam\n",
    "from train import TrainerVaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 128\n",
    "    dataset = 'webcam'\n",
    "    pretrained_path = 'weights/pretrained_parameter.pth'\n",
    "    pretrain = True\n",
    "    lr_ae = 5e-4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "args = Args() # Parsing all the arguments for the training\n",
    "if args.dataset == 'mnist':\n",
    "    dataloader_train, dataloader_test = get_mnist(args)\n",
    "    n_classes = 10\n",
    "else:\n",
    "    dataloader_train, dataloader_test = get_webcam(args)\n",
    "    n_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade = TrainerVaDE(args, device, dataloader_train, dataloader_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the autoencoder...\n",
      "Training Autoencoder... Epoch: 0, Loss: 0.2748433897892634\n",
      "Training Autoencoder... Epoch: 1, Loss: 0.21487239499886832\n",
      "Training Autoencoder... Epoch: 2, Loss: 0.19823907812436423\n",
      "Training Autoencoder... Epoch: 3, Loss: 0.1905296395222346\n",
      "Training Autoencoder... Epoch: 4, Loss: 0.18437669426202774\n",
      "Training Autoencoder... Epoch: 5, Loss: 0.17887309938669205\n",
      "Training Autoencoder... Epoch: 6, Loss: 0.17576623459657034\n",
      "Training Autoencoder... Epoch: 7, Loss: 0.17394153525431952\n",
      "Training Autoencoder... Epoch: 8, Loss: 0.17318113396565119\n",
      "Training Autoencoder... Epoch: 9, Loss: 0.17244506627321243\n",
      "Training Autoencoder... Epoch: 10, Loss: 0.17190323024988174\n",
      "Training Autoencoder... Epoch: 11, Loss: 0.17129253596067429\n",
      "Training Autoencoder... Epoch: 12, Loss: 0.17006355772415796\n",
      "Training Autoencoder... Epoch: 13, Loss: 0.16887133568525314\n",
      "Training Autoencoder... Epoch: 14, Loss: 0.16739669193824133\n",
      "Training Autoencoder... Epoch: 15, Loss: 0.1662129412094752\n",
      "Training Autoencoder... Epoch: 16, Loss: 0.16537409275770187\n",
      "Training Autoencoder... Epoch: 17, Loss: 0.16471853107213974\n",
      "Training Autoencoder... Epoch: 18, Loss: 0.163687693576018\n",
      "Training Autoencoder... Epoch: 19, Loss: 0.16237960755825043\n",
      "Training Autoencoder... Epoch: 20, Loss: 0.16153504451115927\n",
      "Training Autoencoder... Epoch: 21, Loss: 0.16045751174290976\n",
      "Training Autoencoder... Epoch: 22, Loss: 0.15940937399864197\n",
      "Training Autoencoder... Epoch: 23, Loss: 0.15791176507870355\n",
      "Training Autoencoder... Epoch: 24, Loss: 0.15712754428386688\n",
      "Training Autoencoder... Epoch: 25, Loss: 0.15696891645590463\n",
      "Training Autoencoder... Epoch: 26, Loss: 0.15580875426530838\n",
      "Training Autoencoder... Epoch: 27, Loss: 0.1551981841524442\n",
      "Training Autoencoder... Epoch: 28, Loss: 0.15447308123111725\n",
      "Training Autoencoder... Epoch: 29, Loss: 0.15373510867357254\n",
      "Training Autoencoder... Epoch: 30, Loss: 0.1529508257905642\n",
      "Training Autoencoder... Epoch: 31, Loss: 0.151982252796491\n",
      "Training Autoencoder... Epoch: 32, Loss: 0.15121339013179144\n",
      "Training Autoencoder... Epoch: 33, Loss: 0.15039876600106558\n",
      "Training Autoencoder... Epoch: 34, Loss: 0.14932807783285776\n",
      "Training Autoencoder... Epoch: 35, Loss: 0.14846356958150864\n",
      "Training Autoencoder... Epoch: 36, Loss: 0.14766031752030054\n",
      "Training Autoencoder... Epoch: 37, Loss: 0.14686295141776404\n",
      "Training Autoencoder... Epoch: 38, Loss: 0.14662778625885645\n",
      "Training Autoencoder... Epoch: 39, Loss: 0.14526342352231345\n",
      "Training Autoencoder... Epoch: 40, Loss: 0.14474534740050635\n",
      "Training Autoencoder... Epoch: 41, Loss: 0.1437252735098203\n",
      "Training Autoencoder... Epoch: 42, Loss: 0.1437506377696991\n",
      "Training Autoencoder... Epoch: 43, Loss: 0.14248612523078918\n",
      "Training Autoencoder... Epoch: 44, Loss: 0.1420911451180776\n",
      "Training Autoencoder... Epoch: 45, Loss: 0.1414630189538002\n",
      "Training Autoencoder... Epoch: 46, Loss: 0.14127754668394724\n",
      "Training Autoencoder... Epoch: 47, Loss: 0.1409183790286382\n",
      "Training Autoencoder... Epoch: 48, Loss: 0.14040412505467734\n",
      "Training Autoencoder... Epoch: 49, Loss: 0.13996896147727966\n",
      "Training Autoencoder... Epoch: 50, Loss: 0.1397914638121923\n",
      "Training Autoencoder... Epoch: 51, Loss: 0.13911606619755426\n",
      "Training Autoencoder... Epoch: 52, Loss: 0.13808543980121613\n",
      "Training Autoencoder... Epoch: 53, Loss: 0.13796749959389368\n",
      "Training Autoencoder... Epoch: 54, Loss: 0.1377298136552175\n",
      "Training Autoencoder... Epoch: 55, Loss: 0.13725955535968146\n",
      "Training Autoencoder... Epoch: 56, Loss: 0.1371768961350123\n",
      "Training Autoencoder... Epoch: 57, Loss: 0.13648423055807749\n",
      "Training Autoencoder... Epoch: 58, Loss: 0.1363450437784195\n",
      "Training Autoencoder... Epoch: 59, Loss: 0.13608601440985998\n",
      "Training Autoencoder... Epoch: 60, Loss: 0.1355298931399981\n",
      "Training Autoencoder... Epoch: 61, Loss: 0.1350830594698588\n",
      "Training Autoencoder... Epoch: 62, Loss: 0.13503974179426828\n",
      "Training Autoencoder... Epoch: 63, Loss: 0.13501551995674768\n",
      "Training Autoencoder... Epoch: 64, Loss: 0.13482479999462763\n",
      "Training Autoencoder... Epoch: 65, Loss: 0.1344427391886711\n",
      "Training Autoencoder... Epoch: 66, Loss: 0.13358444968859354\n",
      "Training Autoencoder... Epoch: 67, Loss: 0.1340844507018725\n",
      "Training Autoencoder... Epoch: 68, Loss: 0.13367453962564468\n",
      "Training Autoencoder... Epoch: 69, Loss: 0.13366301854451498\n",
      "Training Autoencoder... Epoch: 70, Loss: 0.1326455076535543\n",
      "Training Autoencoder... Epoch: 71, Loss: 0.1326414868235588\n",
      "Training Autoencoder... Epoch: 72, Loss: 0.13276403894027075\n",
      "Training Autoencoder... Epoch: 73, Loss: 0.13257154325644174\n",
      "Training Autoencoder... Epoch: 74, Loss: 0.1314684400955836\n",
      "Training Autoencoder... Epoch: 75, Loss: 0.13160745799541473\n",
      "Training Autoencoder... Epoch: 76, Loss: 0.13126234958569208\n",
      "Training Autoencoder... Epoch: 77, Loss: 0.13070653875668845\n",
      "Training Autoencoder... Epoch: 78, Loss: 0.13144384572903314\n",
      "Training Autoencoder... Epoch: 79, Loss: 0.13094889869292578\n",
      "Training Autoencoder... Epoch: 80, Loss: 0.1311132808526357\n",
      "Training Autoencoder... Epoch: 81, Loss: 0.13076189160346985\n",
      "Training Autoencoder... Epoch: 82, Loss: 0.130266971886158\n",
      "Training Autoencoder... Epoch: 83, Loss: 0.12969875832398733\n",
      "Training Autoencoder... Epoch: 84, Loss: 0.12968175609906515\n",
      "Training Autoencoder... Epoch: 85, Loss: 0.12993756433327994\n",
      "Training Autoencoder... Epoch: 86, Loss: 0.12909593433141708\n",
      "Training Autoencoder... Epoch: 87, Loss: 0.12906069805224737\n",
      "Training Autoencoder... Epoch: 88, Loss: 0.1293355052669843\n",
      "Training Autoencoder... Epoch: 89, Loss: 0.12970000753800073\n",
      "Training Autoencoder... Epoch: 90, Loss: 0.12855359415213266\n",
      "Training Autoencoder... Epoch: 91, Loss: 0.1281170000632604\n",
      "Training Autoencoder... Epoch: 92, Loss: 0.1280661622683207\n",
      "Training Autoencoder... Epoch: 93, Loss: 0.12806039303541183\n",
      "Training Autoencoder... Epoch: 94, Loss: 0.1281602755188942\n",
      "Training Autoencoder... Epoch: 95, Loss: 0.12803528209527335\n",
      "Training Autoencoder... Epoch: 96, Loss: 0.12758049120505652\n",
      "Training Autoencoder... Epoch: 97, Loss: 0.1275481954216957\n",
      "Training Autoencoder... Epoch: 98, Loss: 0.12676991894841194\n",
      "Training Autoencoder... Epoch: 99, Loss: 0.12696141997973123\n",
      "Training Autoencoder... Epoch: 100, Loss: 0.12739993259310722\n",
      "Training Autoencoder... Epoch: 101, Loss: 0.12710768729448318\n",
      "Training Autoencoder... Epoch: 102, Loss: 0.12668799608945847\n",
      "Training Autoencoder... Epoch: 103, Loss: 0.1268816627562046\n",
      "Training Autoencoder... Epoch: 104, Loss: 0.12640034034848213\n",
      "Training Autoencoder... Epoch: 105, Loss: 0.12637636562188467\n",
      "Training Autoencoder... Epoch: 106, Loss: 0.12613975132505098\n",
      "Training Autoencoder... Epoch: 107, Loss: 0.1256894456843535\n",
      "Training Autoencoder... Epoch: 108, Loss: 0.12550634269913039\n",
      "Training Autoencoder... Epoch: 109, Loss: 0.12575278927882513\n",
      "Training Autoencoder... Epoch: 110, Loss: 0.12559664249420166\n",
      "Training Autoencoder... Epoch: 111, Loss: 0.12558752795060477\n",
      "Training Autoencoder... Epoch: 112, Loss: 0.1252896711230278\n",
      "Training Autoencoder... Epoch: 113, Loss: 0.12507417301336923\n",
      "Training Autoencoder... Epoch: 114, Loss: 0.12529725084702173\n",
      "Training Autoencoder... Epoch: 115, Loss: 0.1250646561384201\n",
      "Training Autoencoder... Epoch: 116, Loss: 0.125091432283322\n",
      "Training Autoencoder... Epoch: 117, Loss: 0.12461723759770393\n",
      "Training Autoencoder... Epoch: 118, Loss: 0.12524493411183357\n",
      "Training Autoencoder... Epoch: 119, Loss: 0.12486183146635692\n",
      "Training Autoencoder... Epoch: 120, Loss: 0.1244276911020279\n",
      "Training Autoencoder... Epoch: 121, Loss: 0.12422141929467519\n",
      "Training Autoencoder... Epoch: 122, Loss: 0.12419298539559047\n",
      "Training Autoencoder... Epoch: 123, Loss: 0.12404573584596316\n",
      "Training Autoencoder... Epoch: 124, Loss: 0.12407132610678673\n",
      "Training Autoencoder... Epoch: 125, Loss: 0.12353566661477089\n",
      "Training Autoencoder... Epoch: 126, Loss: 0.12321011473735173\n",
      "Training Autoencoder... Epoch: 127, Loss: 0.12339060256878535\n",
      "Training Autoencoder... Epoch: 128, Loss: 0.12374047935009003\n",
      "Training Autoencoder... Epoch: 129, Loss: 0.12299660841623943\n",
      "Training Autoencoder... Epoch: 130, Loss: 0.12271862601240476\n",
      "Training Autoencoder... Epoch: 131, Loss: 0.12269284203648567\n",
      "Training Autoencoder... Epoch: 132, Loss: 0.12302964304884274\n",
      "Training Autoencoder... Epoch: 133, Loss: 0.12279427299896876\n",
      "Training Autoencoder... Epoch: 134, Loss: 0.1225595014790694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 135, Loss: 0.1232833241422971\n",
      "Training Autoencoder... Epoch: 136, Loss: 0.12246580297748248\n",
      "Training Autoencoder... Epoch: 137, Loss: 0.12236416215697925\n",
      "Training Autoencoder... Epoch: 138, Loss: 0.12258224934339523\n",
      "Training Autoencoder... Epoch: 139, Loss: 0.12203090141216914\n",
      "Training Autoencoder... Epoch: 140, Loss: 0.12200895821054776\n",
      "Training Autoencoder... Epoch: 141, Loss: 0.121670617411534\n",
      "Training Autoencoder... Epoch: 142, Loss: 0.12142673134803772\n",
      "Training Autoencoder... Epoch: 143, Loss: 0.1216173345843951\n",
      "Training Autoencoder... Epoch: 144, Loss: 0.12141937762498856\n",
      "Training Autoencoder... Epoch: 145, Loss: 0.12119235222538312\n",
      "Training Autoencoder... Epoch: 146, Loss: 0.1215141328672568\n",
      "Training Autoencoder... Epoch: 147, Loss: 0.12168754761417706\n",
      "Training Autoencoder... Epoch: 148, Loss: 0.12121113513906796\n",
      "Training Autoencoder... Epoch: 149, Loss: 0.12127822016676267\n",
      "Training Autoencoder... Epoch: 150, Loss: 0.12113465741276741\n",
      "Training Autoencoder... Epoch: 151, Loss: 0.12082699313759804\n",
      "Training Autoencoder... Epoch: 152, Loss: 0.12079358473420143\n",
      "Training Autoencoder... Epoch: 153, Loss: 0.12086363633473714\n",
      "Training Autoencoder... Epoch: 154, Loss: 0.12055046483874321\n",
      "Training Autoencoder... Epoch: 155, Loss: 0.1200667458275954\n",
      "Training Autoencoder... Epoch: 156, Loss: 0.12038816759983699\n",
      "Training Autoencoder... Epoch: 157, Loss: 0.12017900869250298\n",
      "Training Autoencoder... Epoch: 158, Loss: 0.12026733160018921\n",
      "Training Autoencoder... Epoch: 159, Loss: 0.12018126373489697\n",
      "Training Autoencoder... Epoch: 160, Loss: 0.1198558621108532\n",
      "Training Autoencoder... Epoch: 161, Loss: 0.11974712709585826\n",
      "Training Autoencoder... Epoch: 162, Loss: 0.12000000725189845\n",
      "Training Autoencoder... Epoch: 163, Loss: 0.1200870933632056\n",
      "Training Autoencoder... Epoch: 164, Loss: 0.11987169211109479\n",
      "Training Autoencoder... Epoch: 165, Loss: 0.1192803680896759\n",
      "Training Autoencoder... Epoch: 166, Loss: 0.11963722358147304\n",
      "Training Autoencoder... Epoch: 167, Loss: 0.11947323630253474\n",
      "Training Autoencoder... Epoch: 168, Loss: 0.11922723054885864\n",
      "Training Autoencoder... Epoch: 169, Loss: 0.1189145694176356\n",
      "Training Autoencoder... Epoch: 170, Loss: 0.11915186792612076\n",
      "Training Autoencoder... Epoch: 171, Loss: 0.11939757689833641\n",
      "Training Autoencoder... Epoch: 172, Loss: 0.11898702383041382\n",
      "Training Autoencoder... Epoch: 173, Loss: 0.11902764067053795\n",
      "Training Autoencoder... Epoch: 174, Loss: 0.1190081425011158\n",
      "Training Autoencoder... Epoch: 175, Loss: 0.11834621677796046\n",
      "Training Autoencoder... Epoch: 176, Loss: 0.11921352272232373\n",
      "Training Autoencoder... Epoch: 177, Loss: 0.11900621031721433\n",
      "Training Autoencoder... Epoch: 178, Loss: 0.11850214873751004\n",
      "Training Autoencoder... Epoch: 179, Loss: 0.11838045964638393\n",
      "Training Autoencoder... Epoch: 180, Loss: 0.11883612846334775\n",
      "Training Autoencoder... Epoch: 181, Loss: 0.11794304351011912\n",
      "Training Autoencoder... Epoch: 182, Loss: 0.11813365916411082\n",
      "Training Autoencoder... Epoch: 183, Loss: 0.1174188069999218\n",
      "Training Autoencoder... Epoch: 184, Loss: 0.11778032779693604\n",
      "Training Autoencoder... Epoch: 185, Loss: 0.11789770300189654\n",
      "Training Autoencoder... Epoch: 186, Loss: 0.11743621652324994\n",
      "Training Autoencoder... Epoch: 187, Loss: 0.11742549017071724\n",
      "Training Autoencoder... Epoch: 188, Loss: 0.11777170871694882\n",
      "Training Autoencoder... Epoch: 189, Loss: 0.11711336175600688\n",
      "Training Autoencoder... Epoch: 190, Loss: 0.11768274754285812\n",
      "Training Autoencoder... Epoch: 191, Loss: 0.11727768927812576\n",
      "Training Autoencoder... Epoch: 192, Loss: 0.11763685444990794\n",
      "Training Autoencoder... Epoch: 193, Loss: 0.11713657279809316\n",
      "Training Autoencoder... Epoch: 194, Loss: 0.11725559830665588\n",
      "Training Autoencoder... Epoch: 195, Loss: 0.11741435155272484\n",
      "Training Autoencoder... Epoch: 196, Loss: 0.11738251398007075\n",
      "Training Autoencoder... Epoch: 197, Loss: 0.11712177097797394\n",
      "Training Autoencoder... Epoch: 198, Loss: 0.11728493869304657\n",
      "Training Autoencoder... Epoch: 199, Loss: 0.1169063796599706\n",
      "Training Autoencoder... Epoch: 200, Loss: 0.11665183305740356\n",
      "Training Autoencoder... Epoch: 201, Loss: 0.11635696639617284\n",
      "Training Autoencoder... Epoch: 202, Loss: 0.1165220874051253\n",
      "Training Autoencoder... Epoch: 203, Loss: 0.11641147981087367\n",
      "Training Autoencoder... Epoch: 204, Loss: 0.11620527009169261\n",
      "Training Autoencoder... Epoch: 205, Loss: 0.11685861647129059\n",
      "Training Autoencoder... Epoch: 206, Loss: 0.11634736508131027\n",
      "Training Autoencoder... Epoch: 207, Loss: 0.1159111460049947\n",
      "Training Autoencoder... Epoch: 208, Loss: 0.11621763929724693\n",
      "Training Autoencoder... Epoch: 209, Loss: 0.11654558529456456\n",
      "Training Autoencoder... Epoch: 210, Loss: 0.11633003254731496\n",
      "Training Autoencoder... Epoch: 211, Loss: 0.11595305427908897\n",
      "Training Autoencoder... Epoch: 212, Loss: 0.11532232041160266\n",
      "Training Autoencoder... Epoch: 213, Loss: 0.1154652622838815\n",
      "Training Autoencoder... Epoch: 214, Loss: 0.11615520591537158\n",
      "Training Autoencoder... Epoch: 215, Loss: 0.11562701314687729\n",
      "Training Autoencoder... Epoch: 216, Loss: 0.11573594560225804\n",
      "Training Autoencoder... Epoch: 217, Loss: 0.11580387751261394\n",
      "Training Autoencoder... Epoch: 218, Loss: 0.1161539579431216\n",
      "Training Autoencoder... Epoch: 219, Loss: 0.11559694012006123\n",
      "Training Autoencoder... Epoch: 220, Loss: 0.11539270853002866\n",
      "Training Autoencoder... Epoch: 221, Loss: 0.11502016584078471\n",
      "Training Autoencoder... Epoch: 222, Loss: 0.1150331770380338\n",
      "Training Autoencoder... Epoch: 223, Loss: 0.11511400838692983\n",
      "Training Autoencoder... Epoch: 224, Loss: 0.1158211516837279\n",
      "Training Autoencoder... Epoch: 225, Loss: 0.11466328551371892\n",
      "Training Autoencoder... Epoch: 226, Loss: 0.11506699522336324\n",
      "Training Autoencoder... Epoch: 227, Loss: 0.11482987056175868\n",
      "Training Autoencoder... Epoch: 228, Loss: 0.11482091620564461\n",
      "Training Autoencoder... Epoch: 229, Loss: 0.11451040829221408\n",
      "Training Autoencoder... Epoch: 230, Loss: 0.1147507019340992\n",
      "Training Autoencoder... Epoch: 231, Loss: 0.11446762705842654\n",
      "Training Autoencoder... Epoch: 232, Loss: 0.11434671531120937\n",
      "Training Autoencoder... Epoch: 233, Loss: 0.11414550244808197\n",
      "Training Autoencoder... Epoch: 234, Loss: 0.11416810502608617\n",
      "Training Autoencoder... Epoch: 235, Loss: 0.11441397791107495\n",
      "Training Autoencoder... Epoch: 236, Loss: 0.11439203843474388\n",
      "Training Autoencoder... Epoch: 237, Loss: 0.11407562469442685\n",
      "Training Autoencoder... Epoch: 238, Loss: 0.11387831469376881\n",
      "Training Autoencoder... Epoch: 239, Loss: 0.11447075630227725\n",
      "Training Autoencoder... Epoch: 240, Loss: 0.11421184241771698\n",
      "Training Autoencoder... Epoch: 241, Loss: 0.11415774499376614\n",
      "Training Autoencoder... Epoch: 242, Loss: 0.11415696764985721\n",
      "Training Autoencoder... Epoch: 243, Loss: 0.11377288152774175\n",
      "Training Autoencoder... Epoch: 244, Loss: 0.1143253929913044\n",
      "Training Autoencoder... Epoch: 245, Loss: 0.11371116091807683\n",
      "Training Autoencoder... Epoch: 246, Loss: 0.11379850159088771\n",
      "Training Autoencoder... Epoch: 247, Loss: 0.11347829923033714\n",
      "Training Autoencoder... Epoch: 248, Loss: 0.11358468482891719\n",
      "Training Autoencoder... Epoch: 249, Loss: 0.11336746190985043\n",
      "Training Autoencoder... Epoch: 250, Loss: 0.11294836923480034\n",
      "Training Autoencoder... Epoch: 251, Loss: 0.1136953334013621\n",
      "Training Autoencoder... Epoch: 252, Loss: 0.1130383921166261\n",
      "Training Autoencoder... Epoch: 253, Loss: 0.11325854808092117\n",
      "Training Autoencoder... Epoch: 254, Loss: 0.11348574236035347\n",
      "Training Autoencoder... Epoch: 255, Loss: 0.11330455417434375\n",
      "Training Autoencoder... Epoch: 256, Loss: 0.1128703070183595\n",
      "Training Autoencoder... Epoch: 257, Loss: 0.11276304100950559\n",
      "Training Autoencoder... Epoch: 258, Loss: 0.11293891320625941\n",
      "Training Autoencoder... Epoch: 259, Loss: 0.11312917123238246\n",
      "Training Autoencoder... Epoch: 260, Loss: 0.11256642142931621\n",
      "Training Autoencoder... Epoch: 261, Loss: 0.1129624458650748\n",
      "Training Autoencoder... Epoch: 262, Loss: 0.11269356310367584\n",
      "Training Autoencoder... Epoch: 263, Loss: 0.11294769744078319\n",
      "Training Autoencoder... Epoch: 264, Loss: 0.1126999519765377\n",
      "Training Autoencoder... Epoch: 265, Loss: 0.11304672062397003\n",
      "Training Autoencoder... Epoch: 266, Loss: 0.11257728561758995\n",
      "Training Autoencoder... Epoch: 267, Loss: 0.1125307319064935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 268, Loss: 0.11222745602329572\n",
      "Training Autoencoder... Epoch: 269, Loss: 0.11216185614466667\n",
      "Training Autoencoder... Epoch: 270, Loss: 0.11264589180548985\n",
      "Training Autoencoder... Epoch: 271, Loss: 0.11271868273615837\n",
      "Training Autoencoder... Epoch: 272, Loss: 0.11190333962440491\n",
      "Training Autoencoder... Epoch: 273, Loss: 0.11262850711743037\n",
      "Training Autoencoder... Epoch: 274, Loss: 0.112200066447258\n",
      "Training Autoencoder... Epoch: 275, Loss: 0.1120898611843586\n",
      "Training Autoencoder... Epoch: 276, Loss: 0.11214256534973781\n",
      "Training Autoencoder... Epoch: 277, Loss: 0.11259427418311437\n",
      "Training Autoencoder... Epoch: 278, Loss: 0.11215860148270924\n",
      "Training Autoencoder... Epoch: 279, Loss: 0.11217675358057022\n",
      "Training Autoencoder... Epoch: 280, Loss: 0.11139573901891708\n",
      "Training Autoencoder... Epoch: 281, Loss: 0.11165505771835645\n",
      "Training Autoencoder... Epoch: 282, Loss: 0.11147169520457585\n",
      "Training Autoencoder... Epoch: 283, Loss: 0.11177080993851025\n",
      "Training Autoencoder... Epoch: 284, Loss: 0.11170671880245209\n",
      "Training Autoencoder... Epoch: 285, Loss: 0.11174780999620755\n",
      "Training Autoencoder... Epoch: 286, Loss: 0.11117126544316609\n",
      "Training Autoencoder... Epoch: 287, Loss: 0.11131756876905759\n",
      "Training Autoencoder... Epoch: 288, Loss: 0.11173832664887111\n",
      "Training Autoencoder... Epoch: 289, Loss: 0.11166271691521008\n",
      "Training Autoencoder... Epoch: 290, Loss: 0.11183182522654533\n",
      "Training Autoencoder... Epoch: 291, Loss: 0.11175413057208061\n",
      "Training Autoencoder... Epoch: 292, Loss: 0.11130884289741516\n",
      "Training Autoencoder... Epoch: 293, Loss: 0.11115527525544167\n",
      "Training Autoencoder... Epoch: 294, Loss: 0.11147286991278331\n",
      "Training Autoencoder... Epoch: 295, Loss: 0.11069538195927937\n",
      "Training Autoencoder... Epoch: 296, Loss: 0.11160389085610707\n",
      "Training Autoencoder... Epoch: 297, Loss: 0.11088272059957187\n",
      "Training Autoencoder... Epoch: 298, Loss: 0.11054506028691928\n",
      "Training Autoencoder... Epoch: 299, Loss: 0.11074916894237201\n",
      "Training Autoencoder... Epoch: 300, Loss: 0.11106374859809875\n",
      "Training Autoencoder... Epoch: 301, Loss: 0.11079315344492595\n",
      "Training Autoencoder... Epoch: 302, Loss: 0.11069795613487561\n",
      "Training Autoencoder... Epoch: 303, Loss: 0.1104750285545985\n",
      "Training Autoencoder... Epoch: 304, Loss: 0.11048619945844014\n",
      "Training Autoencoder... Epoch: 305, Loss: 0.11079913874467213\n",
      "Training Autoencoder... Epoch: 306, Loss: 0.11077494298418362\n",
      "Training Autoencoder... Epoch: 307, Loss: 0.11082904413342476\n",
      "Training Autoencoder... Epoch: 308, Loss: 0.11063450326522191\n",
      "Training Autoencoder... Epoch: 309, Loss: 0.1105076087017854\n",
      "Training Autoencoder... Epoch: 310, Loss: 0.1103035385409991\n",
      "Training Autoencoder... Epoch: 311, Loss: 0.1102349820236365\n",
      "Training Autoencoder... Epoch: 312, Loss: 0.11021402478218079\n",
      "Training Autoencoder... Epoch: 313, Loss: 0.1103571206331253\n",
      "Training Autoencoder... Epoch: 314, Loss: 0.1104549194375674\n",
      "Training Autoencoder... Epoch: 315, Loss: 0.11041642725467682\n",
      "Training Autoencoder... Epoch: 316, Loss: 0.11024080341060956\n",
      "Training Autoencoder... Epoch: 317, Loss: 0.11013786494731903\n",
      "Training Autoencoder... Epoch: 318, Loss: 0.1102687381207943\n",
      "Training Autoencoder... Epoch: 319, Loss: 0.10934572915236156\n",
      "Training Autoencoder... Epoch: 320, Loss: 0.11002858107288678\n",
      "Training Autoencoder... Epoch: 321, Loss: 0.11004097511370976\n",
      "Training Autoencoder... Epoch: 322, Loss: 0.11012844617168109\n",
      "Training Autoencoder... Epoch: 323, Loss: 0.11032280946771304\n",
      "Training Autoencoder... Epoch: 324, Loss: 0.11034094418088596\n",
      "Training Autoencoder... Epoch: 325, Loss: 0.11012355983257294\n",
      "Training Autoencoder... Epoch: 326, Loss: 0.10990508273243904\n",
      "Training Autoencoder... Epoch: 327, Loss: 0.10969231650233269\n",
      "Training Autoencoder... Epoch: 328, Loss: 0.1099297304948171\n",
      "Training Autoencoder... Epoch: 329, Loss: 0.11000934491554896\n",
      "Training Autoencoder... Epoch: 330, Loss: 0.10949782530466716\n",
      "Training Autoencoder... Epoch: 331, Loss: 0.10959952076276143\n",
      "Training Autoencoder... Epoch: 332, Loss: 0.10935520008206367\n",
      "Training Autoencoder... Epoch: 333, Loss: 0.10970460499326389\n",
      "Training Autoencoder... Epoch: 334, Loss: 0.1096661314368248\n",
      "Training Autoencoder... Epoch: 335, Loss: 0.1095659186442693\n",
      "Training Autoencoder... Epoch: 336, Loss: 0.1096363291144371\n",
      "Training Autoencoder... Epoch: 337, Loss: 0.1094318466881911\n",
      "Training Autoencoder... Epoch: 338, Loss: 0.1094487061103185\n",
      "Training Autoencoder... Epoch: 339, Loss: 0.10973134636878967\n",
      "Training Autoencoder... Epoch: 340, Loss: 0.10922317455212276\n",
      "Training Autoencoder... Epoch: 341, Loss: 0.10890970254937808\n",
      "Training Autoencoder... Epoch: 342, Loss: 0.10950688272714615\n",
      "Training Autoencoder... Epoch: 343, Loss: 0.10878438254197438\n",
      "Training Autoencoder... Epoch: 344, Loss: 0.10913808022936185\n",
      "Training Autoencoder... Epoch: 345, Loss: 0.10898327454924583\n",
      "Training Autoencoder... Epoch: 346, Loss: 0.10879593342542648\n",
      "Training Autoencoder... Epoch: 347, Loss: 0.10900263115763664\n",
      "Training Autoencoder... Epoch: 348, Loss: 0.10893031458059947\n",
      "Training Autoencoder... Epoch: 349, Loss: 0.10915994147459666\n",
      "Training Autoencoder... Epoch: 350, Loss: 0.10862729698419571\n",
      "Training Autoencoder... Epoch: 351, Loss: 0.10883513217171033\n",
      "Training Autoencoder... Epoch: 352, Loss: 0.1084108054637909\n",
      "Training Autoencoder... Epoch: 353, Loss: 0.10875125229358673\n",
      "Training Autoencoder... Epoch: 354, Loss: 0.10878532007336617\n",
      "Training Autoencoder... Epoch: 355, Loss: 0.1088679035504659\n",
      "Training Autoencoder... Epoch: 356, Loss: 0.10858480880657832\n",
      "Training Autoencoder... Epoch: 357, Loss: 0.10876512403289477\n",
      "Training Autoencoder... Epoch: 358, Loss: 0.10870110243558884\n",
      "Training Autoencoder... Epoch: 359, Loss: 0.10872454692920049\n",
      "Training Autoencoder... Epoch: 360, Loss: 0.10892001166939735\n",
      "Training Autoencoder... Epoch: 361, Loss: 0.10850572461883227\n",
      "Training Autoencoder... Epoch: 362, Loss: 0.1083730881412824\n",
      "Training Autoencoder... Epoch: 363, Loss: 0.10833866025010745\n",
      "Training Autoencoder... Epoch: 364, Loss: 0.10851473112901051\n",
      "Training Autoencoder... Epoch: 365, Loss: 0.10843593627214432\n",
      "Training Autoencoder... Epoch: 366, Loss: 0.10816769922773044\n",
      "Training Autoencoder... Epoch: 367, Loss: 0.10811839004357655\n",
      "Training Autoencoder... Epoch: 368, Loss: 0.10835850238800049\n",
      "Training Autoencoder... Epoch: 369, Loss: 0.1077147126197815\n",
      "Training Autoencoder... Epoch: 370, Loss: 0.10773447528481483\n",
      "Training Autoencoder... Epoch: 371, Loss: 0.10837251444657643\n",
      "Training Autoencoder... Epoch: 372, Loss: 0.10830994447072347\n",
      "Training Autoencoder... Epoch: 373, Loss: 0.10822179168462753\n",
      "Training Autoencoder... Epoch: 374, Loss: 0.10823087270061175\n",
      "Training Autoencoder... Epoch: 375, Loss: 0.10787429536382358\n",
      "Training Autoencoder... Epoch: 376, Loss: 0.1084579589466254\n",
      "Training Autoencoder... Epoch: 377, Loss: 0.10783809299270312\n",
      "Training Autoencoder... Epoch: 378, Loss: 0.10806131362915039\n",
      "Training Autoencoder... Epoch: 379, Loss: 0.10828448211153348\n",
      "Training Autoencoder... Epoch: 380, Loss: 0.10765405371785164\n",
      "Training Autoencoder... Epoch: 381, Loss: 0.10833815981944402\n",
      "Training Autoencoder... Epoch: 382, Loss: 0.10798751811186473\n",
      "Training Autoencoder... Epoch: 383, Loss: 0.10829565549890201\n",
      "Training Autoencoder... Epoch: 384, Loss: 0.10813820982972781\n",
      "Training Autoencoder... Epoch: 385, Loss: 0.1076953758796056\n",
      "Training Autoencoder... Epoch: 386, Loss: 0.10749022165934245\n",
      "Training Autoencoder... Epoch: 387, Loss: 0.10745054980119069\n",
      "Training Autoencoder... Epoch: 388, Loss: 0.10733389233549435\n",
      "Training Autoencoder... Epoch: 389, Loss: 0.10788821925719579\n",
      "Training Autoencoder... Epoch: 390, Loss: 0.10800755520661671\n",
      "Training Autoencoder... Epoch: 391, Loss: 0.10806201895078023\n",
      "Training Autoencoder... Epoch: 392, Loss: 0.10774676998456319\n",
      "Training Autoencoder... Epoch: 393, Loss: 0.10705631350477536\n",
      "Training Autoencoder... Epoch: 394, Loss: 0.10742330054442088\n",
      "Training Autoencoder... Epoch: 395, Loss: 0.10738711928327878\n",
      "Training Autoencoder... Epoch: 396, Loss: 0.10701578110456467\n",
      "Training Autoencoder... Epoch: 397, Loss: 0.10740703468521436\n",
      "Training Autoencoder... Epoch: 398, Loss: 0.10741687069336574\n",
      "Training Autoencoder... Epoch: 399, Loss: 0.10647424186269443\n",
      "Training Autoencoder... Epoch: 400, Loss: 0.10736630608638127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 401, Loss: 0.10725672791401546\n",
      "Training Autoencoder... Epoch: 402, Loss: 0.1066914697488149\n",
      "Training Autoencoder... Epoch: 403, Loss: 0.10695317511757214\n",
      "Training Autoencoder... Epoch: 404, Loss: 0.10706148544947307\n",
      "Training Autoencoder... Epoch: 405, Loss: 0.10749708736936252\n",
      "Training Autoencoder... Epoch: 406, Loss: 0.10673957442243893\n",
      "Training Autoencoder... Epoch: 407, Loss: 0.10702826951940854\n",
      "Training Autoencoder... Epoch: 408, Loss: 0.10659423097968102\n",
      "Training Autoencoder... Epoch: 409, Loss: 0.10723995293180148\n",
      "Training Autoencoder... Epoch: 410, Loss: 0.10723600164055824\n",
      "Training Autoencoder... Epoch: 411, Loss: 0.1070500190059344\n",
      "Training Autoencoder... Epoch: 412, Loss: 0.10672352090477943\n",
      "Training Autoencoder... Epoch: 413, Loss: 0.107278178135554\n",
      "Training Autoencoder... Epoch: 414, Loss: 0.10743766526381175\n",
      "Training Autoencoder... Epoch: 415, Loss: 0.10714571550488472\n",
      "Training Autoencoder... Epoch: 416, Loss: 0.10682006428639094\n",
      "Training Autoencoder... Epoch: 417, Loss: 0.10682785386840503\n",
      "Training Autoencoder... Epoch: 418, Loss: 0.10703323284784953\n",
      "Training Autoencoder... Epoch: 419, Loss: 0.10729851325352986\n",
      "Training Autoencoder... Epoch: 420, Loss: 0.10652866959571838\n",
      "Training Autoencoder... Epoch: 421, Loss: 0.10639261454343796\n",
      "Training Autoencoder... Epoch: 422, Loss: 0.10667685295144717\n",
      "Training Autoencoder... Epoch: 423, Loss: 0.10732317591706912\n",
      "Training Autoencoder... Epoch: 424, Loss: 0.10660337035854657\n",
      "Training Autoencoder... Epoch: 425, Loss: 0.1065550446510315\n",
      "Training Autoencoder... Epoch: 426, Loss: 0.10671436414122581\n",
      "Training Autoencoder... Epoch: 427, Loss: 0.10600445543726285\n",
      "Training Autoencoder... Epoch: 428, Loss: 0.1067380669216315\n",
      "Training Autoencoder... Epoch: 429, Loss: 0.10636804873744647\n",
      "Training Autoencoder... Epoch: 430, Loss: 0.1065569097797076\n",
      "Training Autoencoder... Epoch: 431, Loss: 0.10669586683313052\n",
      "Training Autoencoder... Epoch: 432, Loss: 0.10688336814443271\n",
      "Training Autoencoder... Epoch: 433, Loss: 0.10619505246480306\n",
      "Training Autoencoder... Epoch: 434, Loss: 0.10634797314802806\n",
      "Training Autoencoder... Epoch: 435, Loss: 0.10640479624271393\n",
      "Training Autoencoder... Epoch: 436, Loss: 0.10619477927684784\n",
      "Training Autoencoder... Epoch: 437, Loss: 0.10631052156289418\n",
      "Training Autoencoder... Epoch: 438, Loss: 0.10663563137253125\n",
      "Training Autoencoder... Epoch: 439, Loss: 0.10622307906548183\n",
      "Training Autoencoder... Epoch: 440, Loss: 0.10628378639618556\n",
      "Training Autoencoder... Epoch: 441, Loss: 0.10638377939661343\n",
      "Training Autoencoder... Epoch: 442, Loss: 0.10644978781541188\n",
      "Training Autoencoder... Epoch: 443, Loss: 0.10675738379359245\n",
      "Training Autoencoder... Epoch: 444, Loss: 0.10591477404038112\n",
      "Training Autoencoder... Epoch: 445, Loss: 0.10638004168868065\n",
      "Training Autoencoder... Epoch: 446, Loss: 0.10618120928605397\n",
      "Training Autoencoder... Epoch: 447, Loss: 0.10605820640921593\n",
      "Training Autoencoder... Epoch: 448, Loss: 0.1062615526219209\n",
      "Training Autoencoder... Epoch: 449, Loss: 0.10578264792760213\n",
      "Training Autoencoder... Epoch: 450, Loss: 0.10596560562650363\n",
      "Training Autoencoder... Epoch: 451, Loss: 0.10613474994897842\n",
      "Training Autoencoder... Epoch: 452, Loss: 0.10593174646298091\n",
      "Training Autoencoder... Epoch: 453, Loss: 0.1057371733089288\n",
      "Training Autoencoder... Epoch: 454, Loss: 0.10607188194990158\n",
      "Training Autoencoder... Epoch: 455, Loss: 0.1058201715350151\n",
      "Training Autoencoder... Epoch: 456, Loss: 0.10589899619420369\n",
      "Training Autoencoder... Epoch: 457, Loss: 0.1057811106244723\n",
      "Training Autoencoder... Epoch: 458, Loss: 0.10612189148863156\n",
      "Training Autoencoder... Epoch: 459, Loss: 0.10633120809992154\n",
      "Training Autoencoder... Epoch: 460, Loss: 0.10564827298124631\n",
      "Training Autoencoder... Epoch: 461, Loss: 0.10573784386118253\n",
      "Training Autoencoder... Epoch: 462, Loss: 0.10558209692438443\n",
      "Training Autoencoder... Epoch: 463, Loss: 0.10539425040284793\n",
      "Training Autoencoder... Epoch: 464, Loss: 0.10548775270581245\n",
      "Training Autoencoder... Epoch: 465, Loss: 0.10545668626825015\n",
      "Training Autoencoder... Epoch: 466, Loss: 0.10519145677487056\n",
      "Training Autoencoder... Epoch: 467, Loss: 0.10558642819523811\n",
      "Training Autoencoder... Epoch: 468, Loss: 0.10578607891996701\n",
      "Training Autoencoder... Epoch: 469, Loss: 0.10542367274562518\n",
      "Training Autoencoder... Epoch: 470, Loss: 0.1054203137755394\n",
      "Training Autoencoder... Epoch: 471, Loss: 0.10530588279167812\n",
      "Training Autoencoder... Epoch: 472, Loss: 0.10543009887139003\n",
      "Training Autoencoder... Epoch: 473, Loss: 0.10562175263961156\n",
      "Training Autoencoder... Epoch: 474, Loss: 0.1056329719722271\n",
      "Training Autoencoder... Epoch: 475, Loss: 0.10546335950493813\n",
      "Training Autoencoder... Epoch: 476, Loss: 0.10579909384250641\n",
      "Training Autoencoder... Epoch: 477, Loss: 0.10516412059466045\n",
      "Training Autoencoder... Epoch: 478, Loss: 0.10531924789150555\n",
      "Training Autoencoder... Epoch: 479, Loss: 0.10537466655174892\n",
      "Training Autoencoder... Epoch: 480, Loss: 0.10599870110551517\n",
      "Training Autoencoder... Epoch: 481, Loss: 0.10497911895314853\n",
      "Training Autoencoder... Epoch: 482, Loss: 0.10515433177351952\n",
      "Training Autoencoder... Epoch: 483, Loss: 0.10489197944601376\n",
      "Training Autoencoder... Epoch: 484, Loss: 0.10492747401197751\n",
      "Training Autoencoder... Epoch: 485, Loss: 0.10496947045127551\n",
      "Training Autoencoder... Epoch: 486, Loss: 0.10512687141696613\n",
      "Training Autoencoder... Epoch: 487, Loss: 0.10470567395289739\n",
      "Training Autoencoder... Epoch: 488, Loss: 0.10500030716260274\n",
      "Training Autoencoder... Epoch: 489, Loss: 0.10526614512006442\n",
      "Training Autoencoder... Epoch: 490, Loss: 0.1052368755141894\n",
      "Training Autoencoder... Epoch: 491, Loss: 0.10497229794661204\n",
      "Training Autoencoder... Epoch: 492, Loss: 0.10540169353286426\n",
      "Training Autoencoder... Epoch: 493, Loss: 0.10551847517490387\n",
      "Training Autoencoder... Epoch: 494, Loss: 0.10520351429780324\n",
      "Training Autoencoder... Epoch: 495, Loss: 0.10527280221382777\n",
      "Training Autoencoder... Epoch: 496, Loss: 0.10535945246617\n",
      "Training Autoencoder... Epoch: 497, Loss: 0.10529415557781856\n",
      "Training Autoencoder... Epoch: 498, Loss: 0.10473900412519772\n",
      "Training Autoencoder... Epoch: 499, Loss: 0.10466111203034718\n",
      "Training Autoencoder... Epoch: 500, Loss: 0.10519669701655705\n",
      "Training Autoencoder... Epoch: 501, Loss: 0.1044740801056226\n",
      "Training Autoencoder... Epoch: 502, Loss: 0.10492171347141266\n",
      "Training Autoencoder... Epoch: 503, Loss: 0.10490259652336438\n",
      "Training Autoencoder... Epoch: 504, Loss: 0.10493916273117065\n",
      "Training Autoencoder... Epoch: 505, Loss: 0.10491650551557541\n",
      "Training Autoencoder... Epoch: 506, Loss: 0.10509776696562767\n",
      "Training Autoencoder... Epoch: 507, Loss: 0.10504565139611562\n",
      "Training Autoencoder... Epoch: 508, Loss: 0.1045827033619086\n",
      "Training Autoencoder... Epoch: 509, Loss: 0.10457511742909749\n",
      "Training Autoencoder... Epoch: 510, Loss: 0.10443960626920064\n",
      "Training Autoencoder... Epoch: 511, Loss: 0.10484722132484119\n",
      "Training Autoencoder... Epoch: 512, Loss: 0.10431712369124095\n",
      "Training Autoencoder... Epoch: 513, Loss: 0.10496125742793083\n",
      "Training Autoencoder... Epoch: 514, Loss: 0.10449859251578648\n",
      "Training Autoencoder... Epoch: 515, Loss: 0.10440412287910779\n",
      "Training Autoencoder... Epoch: 516, Loss: 0.10463240370154381\n",
      "Training Autoencoder... Epoch: 517, Loss: 0.10459569717446963\n",
      "Training Autoencoder... Epoch: 518, Loss: 0.10468368728955586\n",
      "Training Autoencoder... Epoch: 519, Loss: 0.1044708713889122\n",
      "Training Autoencoder... Epoch: 520, Loss: 0.10436554377277692\n",
      "Training Autoencoder... Epoch: 521, Loss: 0.1048211654027303\n",
      "Training Autoencoder... Epoch: 522, Loss: 0.10479628294706345\n",
      "Training Autoencoder... Epoch: 523, Loss: 0.10402769967913628\n",
      "Training Autoencoder... Epoch: 524, Loss: 0.1046038344502449\n",
      "Training Autoencoder... Epoch: 525, Loss: 0.10466224948565166\n",
      "Training Autoencoder... Epoch: 526, Loss: 0.10411400720477104\n",
      "Training Autoencoder... Epoch: 527, Loss: 0.10465079794327418\n",
      "Training Autoencoder... Epoch: 528, Loss: 0.1044407660762469\n",
      "Training Autoencoder... Epoch: 529, Loss: 0.10442013293504715\n",
      "Training Autoencoder... Epoch: 530, Loss: 0.10470069199800491\n",
      "Training Autoencoder... Epoch: 531, Loss: 0.10448817163705826\n",
      "Training Autoencoder... Epoch: 532, Loss: 0.10388256485263507\n",
      "Training Autoencoder... Epoch: 533, Loss: 0.10413422311345737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 534, Loss: 0.10420417413115501\n",
      "Training Autoencoder... Epoch: 535, Loss: 0.10391948620478313\n",
      "Training Autoencoder... Epoch: 536, Loss: 0.1040929766992728\n",
      "Training Autoencoder... Epoch: 537, Loss: 0.10426182051499684\n",
      "Training Autoencoder... Epoch: 538, Loss: 0.10432386895020802\n",
      "Training Autoencoder... Epoch: 539, Loss: 0.1036177563170592\n",
      "Training Autoencoder... Epoch: 540, Loss: 0.10420646766821544\n",
      "Training Autoencoder... Epoch: 541, Loss: 0.10425379127264023\n",
      "Training Autoencoder... Epoch: 542, Loss: 0.1043223887681961\n",
      "Training Autoencoder... Epoch: 543, Loss: 0.10431193560361862\n",
      "Training Autoencoder... Epoch: 544, Loss: 0.10379138216376305\n",
      "Training Autoencoder... Epoch: 545, Loss: 0.10421908770998319\n",
      "Training Autoencoder... Epoch: 546, Loss: 0.10411672790845235\n",
      "Training Autoencoder... Epoch: 547, Loss: 0.10361056650678317\n",
      "Training Autoencoder... Epoch: 548, Loss: 0.10371999318401019\n",
      "Training Autoencoder... Epoch: 549, Loss: 0.10363862911860149\n",
      "Training Autoencoder... Epoch: 550, Loss: 0.10428483287493388\n",
      "Training Autoencoder... Epoch: 551, Loss: 0.10364594931403796\n",
      "Training Autoencoder... Epoch: 552, Loss: 0.10409045840303104\n",
      "Training Autoencoder... Epoch: 553, Loss: 0.10404221713542938\n",
      "Training Autoencoder... Epoch: 554, Loss: 0.10379630575577418\n",
      "Training Autoencoder... Epoch: 555, Loss: 0.10433872292439143\n",
      "Training Autoencoder... Epoch: 556, Loss: 0.10423081492384274\n",
      "Training Autoencoder... Epoch: 557, Loss: 0.10351610432068507\n",
      "Training Autoencoder... Epoch: 558, Loss: 0.10366350784897804\n",
      "Training Autoencoder... Epoch: 559, Loss: 0.10346620778242747\n",
      "Training Autoencoder... Epoch: 560, Loss: 0.10402048379182816\n",
      "Training Autoencoder... Epoch: 561, Loss: 0.10412625720103581\n",
      "Training Autoencoder... Epoch: 562, Loss: 0.10370482876896858\n",
      "Training Autoencoder... Epoch: 563, Loss: 0.10383751367529233\n",
      "Training Autoencoder... Epoch: 564, Loss: 0.10355691611766815\n",
      "Training Autoencoder... Epoch: 565, Loss: 0.10373483474055926\n",
      "Training Autoencoder... Epoch: 566, Loss: 0.10382848729689916\n",
      "Training Autoencoder... Epoch: 567, Loss: 0.10372916733225186\n",
      "Training Autoencoder... Epoch: 568, Loss: 0.1038293552895387\n",
      "Training Autoencoder... Epoch: 569, Loss: 0.10365346943338712\n",
      "Training Autoencoder... Epoch: 570, Loss: 0.10296737651030223\n",
      "Training Autoencoder... Epoch: 571, Loss: 0.10395387311776479\n",
      "Training Autoencoder... Epoch: 572, Loss: 0.10325099403659503\n",
      "Training Autoencoder... Epoch: 573, Loss: 0.10291022310654323\n",
      "Training Autoencoder... Epoch: 574, Loss: 0.10381560524304707\n",
      "Training Autoencoder... Epoch: 575, Loss: 0.10304689407348633\n",
      "Training Autoencoder... Epoch: 576, Loss: 0.10319483652710915\n",
      "Training Autoencoder... Epoch: 577, Loss: 0.10337944701313972\n",
      "Training Autoencoder... Epoch: 578, Loss: 0.10348522787292798\n",
      "Training Autoencoder... Epoch: 579, Loss: 0.10368522504965465\n",
      "Training Autoencoder... Epoch: 580, Loss: 0.10297238950928052\n",
      "Training Autoencoder... Epoch: 581, Loss: 0.1035516473154227\n",
      "Training Autoencoder... Epoch: 582, Loss: 0.10386484861373901\n",
      "Training Autoencoder... Epoch: 583, Loss: 0.10377957423528035\n",
      "Training Autoencoder... Epoch: 584, Loss: 0.10393277804056804\n",
      "Training Autoencoder... Epoch: 585, Loss: 0.10322721550861995\n",
      "Training Autoencoder... Epoch: 586, Loss: 0.10355411594112714\n",
      "Training Autoencoder... Epoch: 587, Loss: 0.10344720880190532\n",
      "Training Autoencoder... Epoch: 588, Loss: 0.10394073277711868\n",
      "Training Autoencoder... Epoch: 589, Loss: 0.10357165709137917\n",
      "Training Autoencoder... Epoch: 590, Loss: 0.10321729133526485\n",
      "Training Autoencoder... Epoch: 591, Loss: 0.10375576714674632\n",
      "Training Autoencoder... Epoch: 592, Loss: 0.10301007827123006\n",
      "Training Autoencoder... Epoch: 593, Loss: 0.10302000492811203\n",
      "Training Autoencoder... Epoch: 594, Loss: 0.10335361336668332\n",
      "Training Autoencoder... Epoch: 595, Loss: 0.10348426798979442\n",
      "Training Autoencoder... Epoch: 596, Loss: 0.10360556095838547\n",
      "Training Autoencoder... Epoch: 597, Loss: 0.10321615760525067\n",
      "Training Autoencoder... Epoch: 598, Loss: 0.10335211952527364\n",
      "Training Autoencoder... Epoch: 599, Loss: 0.10309487581253052\n",
      "Training Autoencoder... Epoch: 600, Loss: 0.10322730367382367\n",
      "Training Autoencoder... Epoch: 601, Loss: 0.1028444692492485\n",
      "Training Autoencoder... Epoch: 602, Loss: 0.1035992627342542\n",
      "Training Autoencoder... Epoch: 603, Loss: 0.10326091945171356\n",
      "Training Autoencoder... Epoch: 604, Loss: 0.10311287268996239\n",
      "Training Autoencoder... Epoch: 605, Loss: 0.10346753771106403\n",
      "Training Autoencoder... Epoch: 606, Loss: 0.10360515614350636\n",
      "Training Autoencoder... Epoch: 607, Loss: 0.10290622090299924\n",
      "Training Autoencoder... Epoch: 608, Loss: 0.10303192585706711\n",
      "Training Autoencoder... Epoch: 609, Loss: 0.10285952190558116\n",
      "Training Autoencoder... Epoch: 610, Loss: 0.10355450709660848\n",
      "Training Autoencoder... Epoch: 611, Loss: 0.10282563418149948\n",
      "Training Autoencoder... Epoch: 612, Loss: 0.10261560852328937\n",
      "Training Autoencoder... Epoch: 613, Loss: 0.10326970368623734\n",
      "Training Autoencoder... Epoch: 614, Loss: 0.10250964015722275\n",
      "Training Autoencoder... Epoch: 615, Loss: 0.10299089675148328\n",
      "Training Autoencoder... Epoch: 616, Loss: 0.1025138609111309\n",
      "Training Autoencoder... Epoch: 617, Loss: 0.10282731180389722\n",
      "Training Autoencoder... Epoch: 618, Loss: 0.10282021388411522\n",
      "Training Autoencoder... Epoch: 619, Loss: 0.10292612016201019\n",
      "Training Autoencoder... Epoch: 620, Loss: 0.10323500757416089\n",
      "Training Autoencoder... Epoch: 621, Loss: 0.10260468845566113\n",
      "Training Autoencoder... Epoch: 622, Loss: 0.1028565304974715\n",
      "Training Autoencoder... Epoch: 623, Loss: 0.10295493031541507\n",
      "Training Autoencoder... Epoch: 624, Loss: 0.10287963102261226\n",
      "Training Autoencoder... Epoch: 625, Loss: 0.10290692249933879\n",
      "Training Autoencoder... Epoch: 626, Loss: 0.10335529098908107\n",
      "Training Autoencoder... Epoch: 627, Loss: 0.1029896376033624\n",
      "Training Autoencoder... Epoch: 628, Loss: 0.10287328064441681\n",
      "Training Autoencoder... Epoch: 629, Loss: 0.10306256885329883\n",
      "Training Autoencoder... Epoch: 630, Loss: 0.10272730017701785\n",
      "Training Autoencoder... Epoch: 631, Loss: 0.10289484759171803\n",
      "Training Autoencoder... Epoch: 632, Loss: 0.10255463421344757\n",
      "Training Autoencoder... Epoch: 633, Loss: 0.10268443698684375\n",
      "Training Autoencoder... Epoch: 634, Loss: 0.10274147366484006\n",
      "Training Autoencoder... Epoch: 635, Loss: 0.10262347509463628\n",
      "Training Autoencoder... Epoch: 636, Loss: 0.10246391966938972\n",
      "Training Autoencoder... Epoch: 637, Loss: 0.10231155157089233\n",
      "Training Autoencoder... Epoch: 638, Loss: 0.10256680597861607\n",
      "Training Autoencoder... Epoch: 639, Loss: 0.10251921291152637\n",
      "Training Autoencoder... Epoch: 640, Loss: 0.10231439769268036\n",
      "Training Autoencoder... Epoch: 641, Loss: 0.10282489409049352\n",
      "Training Autoencoder... Epoch: 642, Loss: 0.10248821725447972\n",
      "Training Autoencoder... Epoch: 643, Loss: 0.10264575605591138\n",
      "Training Autoencoder... Epoch: 644, Loss: 0.10222763195633888\n",
      "Training Autoencoder... Epoch: 645, Loss: 0.10231123988827069\n",
      "Training Autoencoder... Epoch: 646, Loss: 0.10225188359618187\n",
      "Training Autoencoder... Epoch: 647, Loss: 0.10238403330246608\n",
      "Training Autoencoder... Epoch: 648, Loss: 0.10274372746547063\n",
      "Training Autoencoder... Epoch: 649, Loss: 0.10174970452984174\n",
      "Training Autoencoder... Epoch: 650, Loss: 0.10284375896056493\n",
      "Training Autoencoder... Epoch: 651, Loss: 0.10257282728950183\n",
      "Training Autoencoder... Epoch: 652, Loss: 0.1024859497944514\n",
      "Training Autoencoder... Epoch: 653, Loss: 0.1024887462457021\n",
      "Training Autoencoder... Epoch: 654, Loss: 0.10236311455567677\n",
      "Training Autoencoder... Epoch: 655, Loss: 0.10282723978161812\n",
      "Training Autoencoder... Epoch: 656, Loss: 0.10218069826563199\n",
      "Training Autoencoder... Epoch: 657, Loss: 0.10266933341821034\n",
      "Training Autoencoder... Epoch: 658, Loss: 0.10233059898018837\n",
      "Training Autoencoder... Epoch: 659, Loss: 0.10256075983246167\n",
      "Training Autoencoder... Epoch: 660, Loss: 0.10183487335840861\n",
      "Training Autoencoder... Epoch: 661, Loss: 0.10267441719770432\n",
      "Training Autoencoder... Epoch: 662, Loss: 0.10231011733412743\n",
      "Training Autoencoder... Epoch: 663, Loss: 0.10192662477493286\n",
      "Training Autoencoder... Epoch: 664, Loss: 0.10188930481672287\n",
      "Training Autoencoder... Epoch: 665, Loss: 0.10231480623284976\n",
      "Training Autoencoder... Epoch: 666, Loss: 0.10217289750774701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 667, Loss: 0.10236574585239093\n",
      "Training Autoencoder... Epoch: 668, Loss: 0.10227387771010399\n",
      "Training Autoencoder... Epoch: 669, Loss: 0.10198258111874263\n",
      "Training Autoencoder... Epoch: 670, Loss: 0.10219637552897136\n",
      "Training Autoencoder... Epoch: 671, Loss: 0.10222911089658737\n",
      "Training Autoencoder... Epoch: 672, Loss: 0.10227112968762715\n",
      "Training Autoencoder... Epoch: 673, Loss: 0.10215850298603375\n",
      "Training Autoencoder... Epoch: 674, Loss: 0.1020715559522311\n",
      "Training Autoencoder... Epoch: 675, Loss: 0.10249489545822144\n",
      "Training Autoencoder... Epoch: 676, Loss: 0.10217417155702908\n",
      "Training Autoencoder... Epoch: 677, Loss: 0.10198897495865822\n",
      "Training Autoencoder... Epoch: 678, Loss: 0.1025685506562392\n",
      "Training Autoencoder... Epoch: 679, Loss: 0.10239346697926521\n",
      "Training Autoencoder... Epoch: 680, Loss: 0.10211040079593658\n",
      "Training Autoencoder... Epoch: 681, Loss: 0.10202111427982648\n",
      "Training Autoencoder... Epoch: 682, Loss: 0.10259475807348888\n",
      "Training Autoencoder... Epoch: 683, Loss: 0.10261563087503116\n",
      "Training Autoencoder... Epoch: 684, Loss: 0.1026947982609272\n",
      "Training Autoencoder... Epoch: 685, Loss: 0.10237112889687221\n",
      "Training Autoencoder... Epoch: 686, Loss: 0.10208010425170262\n",
      "Training Autoencoder... Epoch: 687, Loss: 0.10195692380269368\n",
      "Training Autoencoder... Epoch: 688, Loss: 0.10225426902373631\n",
      "Training Autoencoder... Epoch: 689, Loss: 0.10236925880114238\n",
      "Training Autoencoder... Epoch: 690, Loss: 0.10185366372267406\n",
      "Training Autoencoder... Epoch: 691, Loss: 0.10193828617533048\n",
      "Training Autoencoder... Epoch: 692, Loss: 0.10196563725670178\n",
      "Training Autoencoder... Epoch: 693, Loss: 0.1018783338367939\n",
      "Training Autoencoder... Epoch: 694, Loss: 0.10214660689234734\n",
      "Training Autoencoder... Epoch: 695, Loss: 0.10180489346385002\n",
      "Training Autoencoder... Epoch: 696, Loss: 0.10167893767356873\n",
      "Training Autoencoder... Epoch: 697, Loss: 0.10181368763248126\n",
      "Training Autoencoder... Epoch: 698, Loss: 0.10226347794135411\n",
      "Training Autoencoder... Epoch: 699, Loss: 0.1022651195526123\n",
      "Training Autoencoder... Epoch: 700, Loss: 0.10166538755098979\n",
      "Training Autoencoder... Epoch: 701, Loss: 0.10172661766409874\n",
      "Training Autoencoder... Epoch: 702, Loss: 0.1020810417830944\n",
      "Training Autoencoder... Epoch: 703, Loss: 0.10221931338310242\n",
      "Training Autoencoder... Epoch: 704, Loss: 0.10164009407162666\n",
      "Training Autoencoder... Epoch: 705, Loss: 0.1018819622695446\n",
      "Training Autoencoder... Epoch: 706, Loss: 0.10155957688887914\n",
      "Training Autoencoder... Epoch: 707, Loss: 0.10241999104619026\n",
      "Training Autoencoder... Epoch: 708, Loss: 0.10192614669601123\n",
      "Training Autoencoder... Epoch: 709, Loss: 0.1014960582057635\n",
      "Training Autoencoder... Epoch: 710, Loss: 0.10216594114899635\n",
      "Training Autoencoder... Epoch: 711, Loss: 0.1020078311363856\n",
      "Training Autoencoder... Epoch: 712, Loss: 0.10206723089019458\n",
      "Training Autoencoder... Epoch: 713, Loss: 0.10138818124930064\n",
      "Training Autoencoder... Epoch: 714, Loss: 0.1019597960015138\n",
      "Training Autoencoder... Epoch: 715, Loss: 0.10169680416584015\n",
      "Training Autoencoder... Epoch: 716, Loss: 0.10178601493438084\n",
      "Training Autoencoder... Epoch: 717, Loss: 0.10165286188324292\n",
      "Training Autoencoder... Epoch: 718, Loss: 0.10164024432500203\n"
     ]
    }
   ],
   "source": [
    "vade.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(vade.loss_ae)\n",
    "\n",
    "def plot_loss(values, metric, dataset):\n",
    "    plt.plot(np.arange(len(values)), values, c='k', label=metric)\n",
    "    plt.title('Autencoder {}'.format(metric))\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Epoch')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('weights/autoencoder_{}_{}'.format(metric, dataset))\n",
    "\n",
    "plot_loss(loss, 'Loss', args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "if args.dataset == 'webcam':\n",
    "    classes = ['back_pack',\n",
    "                'bike',\n",
    "                'bike_helmet',\n",
    "                'bookcase',\n",
    "                'bottle',\n",
    "                'calculator',\n",
    "                'desk_chair',\n",
    "                'desk_lamp',\n",
    "                'desktop_computer',\n",
    "                'file_cabinet',\n",
    "                'headphones',\n",
    "                'keyboard',\n",
    "                'laptop_computer',\n",
    "                'letter_tray',\n",
    "                'mobile_phone',\n",
    "                'monitor',\n",
    "                'mouse',\n",
    "                'mug',\n",
    "                'paper_notebook',\n",
    "                'pen',\n",
    "                'phone',\n",
    "                'printer',\n",
    "                'projector',\n",
    "                'punchers',\n",
    "                'ring_binder',\n",
    "                'ruler',\n",
    "                'scissors',\n",
    "                'speaker',\n",
    "                'stapler',\n",
    "                'tape_dispenser',\n",
    "                'trash_can']\n",
    "else:\n",
    "    classes = ['0',\n",
    "               '1',\n",
    "               '2',\n",
    "               '3',\n",
    "               '4',\n",
    "               '5',\n",
    "               '6',\n",
    "               '7',\n",
    "               '8',\n",
    "               '9']\n",
    "\n",
    "\n",
    "def get_latent_space(dataloader, z_dim, model, device, ftr_ext=None):\n",
    "    z = torch.zeros((1, z_dim)).float().to(device)\n",
    "    y = torch.zeros((1)).long().to(device)\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device).float(), label.to(device).long()\n",
    "            if ftr_ext is not None:\n",
    "                img = ftr_ext(img); img = img.detach()\n",
    "\n",
    "            z_l = model.encode(img)\n",
    "            y = torch.cat((y, label), dim=0)\n",
    "            z = torch.cat((z, z_l), dim=0)\n",
    "    return z[1:], y[1:]\n",
    "\n",
    "\n",
    "def plot_tsne(X_embedded, y, ticks, dataset):\n",
    "    f, ax1 = plt.subplots(1, 1, sharey=True, figsize=(15,10))\n",
    "\n",
    "    cmap = plt.get_cmap('jet', 31)\n",
    "\n",
    "\n",
    "    cax = ax1.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y.numpy(),\n",
    "                      s=15, cmap=cmap)\n",
    "\n",
    "    cbar = f.colorbar(cax, ticks=np.linspace(0,30,31))\n",
    "    cbar.ax.set_yticklabels(ticks)\n",
    "\n",
    "    ax1.xaxis.set_visible(False)\n",
    "    ax1.yaxis.set_visible(False)\n",
    "    plt.savefig('weights/autoencoder_tsne_{}'.format(dataset))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "model = vade.autoencoder\n",
    "ftr_ext = vade.feature_extractor\n",
    "z, y = get_latent_space(dataloader_train, z_dim, model, device, ftr_ext)\n",
    "z, y = z.cpu(), y.cpu()\n",
    "z_embedded = TSNE(n_components=2).fit_transform(z.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(z_embedded, y, classes, args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
