{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "from preprocess import get_mnist, get_webcam\n",
    "from train import TrainerVaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 128\n",
    "    lr = 1e-5\n",
    "    dataset = 'webcam'\n",
    "    pretrained_path = 'weights/pretrained_parameter.pth'\n",
    "    patience = 50\n",
    "    pretrain = True\n",
    "    epochs = 200\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "args = Args() # Parsing all the arguments for the training\n",
    "if args.dataset == 'mnist':\n",
    "    dataloader = get_mnist(batch_size=args.batch_size)\n",
    "    n_classes = 10\n",
    "else:\n",
    "    dataloader = get_webcam(batch_size=args.batch_size)\n",
    "    n_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade = TrainerVaDE(args, device, dataloader, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'webcam':\n",
    "    classes = ['back_pack',\n",
    "                'bike',\n",
    "                'bike_helmet',\n",
    "                'bookcase',\n",
    "                'bottle',\n",
    "                'calculator',\n",
    "                'desk_chair',\n",
    "                'desk_lamp',\n",
    "                'desktop_computer',\n",
    "                'file_cabinet',\n",
    "                'headphones',\n",
    "                'keyboard',\n",
    "                'laptop_computer',\n",
    "                'letter_tray',\n",
    "                'mobile_phone',\n",
    "                'monitor',\n",
    "                'mouse',\n",
    "                'mug',\n",
    "                'paper_notebook',\n",
    "                'pen',\n",
    "                'phone',\n",
    "                'printer',\n",
    "                'projector',\n",
    "                'punchers',\n",
    "                'ring_binder',\n",
    "                'ruler',\n",
    "                'scissors',\n",
    "                'speaker',\n",
    "                'stapler',\n",
    "                'tape_dispenser',\n",
    "                'trash_can']\n",
    "else:\n",
    "    classes = ['0',\n",
    "               '1',\n",
    "               '2',\n",
    "               '3',\n",
    "               '4',\n",
    "               '5',\n",
    "               '6',\n",
    "               '7',\n",
    "               '8',\n",
    "               '9']\n",
    "\n",
    "\n",
    "def get_latent_space(dataloader, z_dim, model, device, ftr_ext=None):\n",
    "    z = torch.zeros((1, z_dim)).float().to(device)\n",
    "    y = torch.zeros((1)).long().to(device)\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device).float(), label.to(device).long()\n",
    "            if ftr_ext is not None:\n",
    "                img = ftr_ext(img); img = img.detach()\n",
    "\n",
    "            mu, log_var = model.encode(img)\n",
    "            z_l = model.reparameterize(mu, log_var)\n",
    "            y = torch.cat((y, label), dim=0)\n",
    "            z = torch.cat((z, z_l), dim=0)\n",
    "    return z[1:], y[1:]\n",
    "\n",
    "\n",
    "def plot_tsne(X_embedded, y, ticks):\n",
    "    f, ax1 = plt.subplots(1, 1, sharey=True, figsize=(15,5))\n",
    "\n",
    "    cmap = plt.get_cmap('jet', 31)\n",
    "\n",
    "\n",
    "    cax = ax1.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y.numpy(),\n",
    "                      s=15, cmap=cmap)\n",
    "\n",
    "    cbar = f.colorbar(cax, ticks=np.linspace(0,30,31))\n",
    "    cbar.ax.set_yticklabels(ticks)\n",
    "\n",
    "    ax1.xaxis.set_visible(False)\n",
    "    ax1.yaxis.set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VaDE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VaDE... Epoch: 0, Loss: 13285.23353794643\n",
      "Testing VaDE... Epoch: 0, Loss: 13223.004185267857, Acc: 39.119496855345915\n",
      "Training VaDE... Epoch: 1, Loss: 13232.789481026786\n",
      "Testing VaDE... Epoch: 1, Loss: 13080.526785714286, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 2, Loss: 13095.9482421875\n",
      "Testing VaDE... Epoch: 2, Loss: 13018.60267857143, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 3, Loss: 13031.666573660714\n",
      "Testing VaDE... Epoch: 3, Loss: 12960.36216517857, Acc: 39.119496855345915\n",
      "Training VaDE... Epoch: 4, Loss: 12859.229910714286\n",
      "Testing VaDE... Epoch: 4, Loss: 12798.50892857143, Acc: 38.86792452830189\n",
      "Training VaDE... Epoch: 5, Loss: 12747.818777901786\n",
      "Testing VaDE... Epoch: 5, Loss: 12743.317103794643, Acc: 39.24528301886793\n",
      "Training VaDE... Epoch: 6, Loss: 12712.742466517857\n",
      "Testing VaDE... Epoch: 6, Loss: 12685.273716517857, Acc: 38.86792452830189\n",
      "Training VaDE... Epoch: 7, Loss: 12639.769949776786\n",
      "Testing VaDE... Epoch: 7, Loss: 12514.07017299107, Acc: 39.119496855345915\n",
      "Training VaDE... Epoch: 8, Loss: 12486.643973214286\n",
      "Testing VaDE... Epoch: 8, Loss: 12472.860909598214, Acc: 40.0\n",
      "Training VaDE... Epoch: 9, Loss: 12371.490792410714\n",
      "Testing VaDE... Epoch: 9, Loss: 12339.453822544643, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 10, Loss: 12372.482003348214\n",
      "Testing VaDE... Epoch: 10, Loss: 12254.45786830357, Acc: 38.9937106918239\n",
      "Training VaDE... Epoch: 11, Loss: 12205.082449776786\n",
      "Testing VaDE... Epoch: 11, Loss: 12173.5283203125, Acc: 40.75471698113208\n",
      "Training VaDE... Epoch: 12, Loss: 12123.142996651786\n",
      "Testing VaDE... Epoch: 12, Loss: 12107.02845982143, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 13, Loss: 12061.972377232143\n",
      "Testing VaDE... Epoch: 13, Loss: 12022.750697544643, Acc: 37.9874213836478\n",
      "Training VaDE... Epoch: 14, Loss: 11942.876395089286\n",
      "Testing VaDE... Epoch: 14, Loss: 11944.171316964286, Acc: 38.49056603773585\n",
      "Training VaDE... Epoch: 15, Loss: 11887.292271205357\n",
      "Testing VaDE... Epoch: 15, Loss: 11794.814453125, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 16, Loss: 11785.910574776786\n",
      "Testing VaDE... Epoch: 16, Loss: 11684.13392857143, Acc: 38.36477987421384\n",
      "Training VaDE... Epoch: 17, Loss: 11670.45717075893\n",
      "Testing VaDE... Epoch: 17, Loss: 11664.80161830357, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 18, Loss: 11599.732003348214\n",
      "Testing VaDE... Epoch: 18, Loss: 11555.88978794643, Acc: 39.37106918238994\n",
      "Training VaDE... Epoch: 19, Loss: 11526.656808035714\n",
      "Testing VaDE... Epoch: 19, Loss: 11445.078683035714, Acc: 39.37106918238994\n",
      "Training VaDE... Epoch: 20, Loss: 11400.32435825893\n",
      "Testing VaDE... Epoch: 20, Loss: 11396.234654017857, Acc: 40.125786163522015\n",
      "Training VaDE... Epoch: 21, Loss: 11298.251395089286\n",
      "Testing VaDE... Epoch: 21, Loss: 11296.122628348214, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 22, Loss: 11243.977260044643\n",
      "Testing VaDE... Epoch: 22, Loss: 11165.882533482143, Acc: 40.0\n",
      "Training VaDE... Epoch: 23, Loss: 11129.417271205357\n",
      "Testing VaDE... Epoch: 23, Loss: 11160.173409598214, Acc: 39.37106918238994\n",
      "Training VaDE... Epoch: 24, Loss: 11139.51939174107\n",
      "Testing VaDE... Epoch: 24, Loss: 11062.2265625, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 25, Loss: 11032.345145089286\n",
      "Testing VaDE... Epoch: 25, Loss: 10933.384207589286, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 26, Loss: 10924.425223214286\n",
      "Testing VaDE... Epoch: 26, Loss: 10900.779017857143, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 27, Loss: 10885.364676339286\n",
      "Testing VaDE... Epoch: 27, Loss: 10823.324637276786, Acc: 39.24528301886793\n",
      "Training VaDE... Epoch: 28, Loss: 10810.523856026786\n",
      "Testing VaDE... Epoch: 28, Loss: 10765.763113839286, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 29, Loss: 10734.27622767857\n",
      "Testing VaDE... Epoch: 29, Loss: 10655.156668526786, Acc: 39.87421383647799\n",
      "Training VaDE... Epoch: 30, Loss: 10647.7822265625\n",
      "Testing VaDE... Epoch: 30, Loss: 10616.599190848214, Acc: 38.49056603773585\n",
      "Training VaDE... Epoch: 31, Loss: 10530.56068638393\n",
      "Testing VaDE... Epoch: 31, Loss: 10511.18178013393, Acc: 38.36477987421384\n",
      "Training VaDE... Epoch: 32, Loss: 10477.6240234375\n",
      "Testing VaDE... Epoch: 32, Loss: 10455.779575892857, Acc: 39.24528301886793\n",
      "Training VaDE... Epoch: 33, Loss: 10425.8984375\n",
      "Testing VaDE... Epoch: 33, Loss: 10372.17075892857, Acc: 39.37106918238994\n",
      "Training VaDE... Epoch: 34, Loss: 10350.766322544643\n",
      "Testing VaDE... Epoch: 34, Loss: 10317.2119140625, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 35, Loss: 10257.850864955357\n",
      "Testing VaDE... Epoch: 35, Loss: 10258.5244140625, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 36, Loss: 10236.91196986607\n",
      "Testing VaDE... Epoch: 36, Loss: 10193.67787388393, Acc: 39.37106918238994\n",
      "Training VaDE... Epoch: 37, Loss: 10174.014927455357\n",
      "Testing VaDE... Epoch: 37, Loss: 10164.06654575893, Acc: 39.119496855345915\n",
      "Training VaDE... Epoch: 38, Loss: 10081.150390625\n",
      "Testing VaDE... Epoch: 38, Loss: 10094.110909598214, Acc: 38.86792452830189\n",
      "Training VaDE... Epoch: 39, Loss: 10072.10142299107\n",
      "Testing VaDE... Epoch: 39, Loss: 10028.28501674107, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 40, Loss: 9963.936244419643\n",
      "Testing VaDE... Epoch: 40, Loss: 9914.97865513393, Acc: 38.9937106918239\n",
      "Training VaDE... Epoch: 41, Loss: 9906.09193638393\n",
      "Testing VaDE... Epoch: 41, Loss: 9873.61830357143, Acc: 38.86792452830189\n",
      "Training VaDE... Epoch: 42, Loss: 9876.785853794643\n",
      "Testing VaDE... Epoch: 42, Loss: 9829.7529296875, Acc: 38.742138364779876\n",
      "Training VaDE... Epoch: 43, Loss: 9776.46763392857\n",
      "Testing VaDE... Epoch: 43, Loss: 9739.14536830357, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 44, Loss: 9811.40150669643\n",
      "Testing VaDE... Epoch: 44, Loss: 9770.521484375, Acc: 39.49685534591195\n",
      "Training VaDE... Epoch: 45, Loss: 9699.175502232143\n",
      "Testing VaDE... Epoch: 45, Loss: 9704.312081473214, Acc: 38.86792452830189\n",
      "Training VaDE... Epoch: 46, Loss: 9624.562779017857\n",
      "Testing VaDE... Epoch: 46, Loss: 9635.360630580357, Acc: 38.61635220125786\n",
      "Training VaDE... Epoch: 47, Loss: 9624.0263671875\n",
      "Testing VaDE... Epoch: 47, Loss: 9540.72963169643, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 48, Loss: 9522.436104910714\n",
      "Testing VaDE... Epoch: 48, Loss: 9537.658482142857, Acc: 38.9937106918239\n",
      "Training VaDE... Epoch: 49, Loss: 9552.411411830357\n",
      "Testing VaDE... Epoch: 49, Loss: 9445.187081473214, Acc: 37.9874213836478\n",
      "Training VaDE... Epoch: 50, Loss: 9442.65345982143\n",
      "Testing VaDE... Epoch: 50, Loss: 9480.335379464286, Acc: 38.61635220125786\n",
      "Training VaDE... Epoch: 51, Loss: 9421.160574776786\n",
      "Testing VaDE... Epoch: 51, Loss: 9393.104213169643, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 52, Loss: 9350.415597098214\n",
      "Testing VaDE... Epoch: 52, Loss: 9355.89634486607, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 53, Loss: 9349.521065848214\n",
      "Testing VaDE... Epoch: 53, Loss: 9313.135463169643, Acc: 37.35849056603773\n",
      "Training VaDE... Epoch: 54, Loss: 9288.09974888393\n",
      "Testing VaDE... Epoch: 54, Loss: 9303.02134486607, Acc: 39.24528301886793\n",
      "Training VaDE... Epoch: 55, Loss: 9319.35756138393\n",
      "Testing VaDE... Epoch: 55, Loss: 9204.761300223214, Acc: 37.9874213836478\n",
      "Training VaDE... Epoch: 56, Loss: 9196.767020089286\n",
      "Testing VaDE... Epoch: 56, Loss: 9183.165597098214, Acc: 38.113207547169814\n",
      "Training VaDE... Epoch: 57, Loss: 9185.1005859375\n",
      "Testing VaDE... Epoch: 57, Loss: 9170.82924107143, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 58, Loss: 9129.035574776786\n",
      "Testing VaDE... Epoch: 58, Loss: 9119.309849330357, Acc: 38.113207547169814\n",
      "Training VaDE... Epoch: 59, Loss: 9067.871372767857\n",
      "Testing VaDE... Epoch: 59, Loss: 9105.59291294643, Acc: 38.49056603773585\n",
      "Training VaDE... Epoch: 60, Loss: 9087.7802734375\n",
      "Testing VaDE... Epoch: 60, Loss: 9044.52036830357, Acc: 38.9937106918239\n",
      "Training VaDE... Epoch: 61, Loss: 9081.273856026786\n",
      "Testing VaDE... Epoch: 61, Loss: 9019.374720982143, Acc: 38.23899371069183\n",
      "Training VaDE... Epoch: 62, Loss: 9040.529575892857\n",
      "Testing VaDE... Epoch: 62, Loss: 9021.448660714286, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 63, Loss: 9001.79185267857\n",
      "Testing VaDE... Epoch: 63, Loss: 8935.827706473214, Acc: 37.61006289308176\n",
      "Training VaDE... Epoch: 64, Loss: 8951.80078125\n",
      "Testing VaDE... Epoch: 64, Loss: 8950.18568638393, Acc: 36.855345911949684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VaDE... Epoch: 65, Loss: 8926.201171875\n",
      "Testing VaDE... Epoch: 65, Loss: 8892.917410714286, Acc: 37.735849056603776\n",
      "Training VaDE... Epoch: 66, Loss: 8874.125279017857\n",
      "Testing VaDE... Epoch: 66, Loss: 8894.3525390625, Acc: 37.23270440251572\n",
      "Training VaDE... Epoch: 67, Loss: 8809.151646205357\n",
      "Testing VaDE... Epoch: 67, Loss: 8856.907924107143, Acc: 37.61006289308176\n",
      "Training VaDE... Epoch: 68, Loss: 8830.72349330357\n",
      "Testing VaDE... Epoch: 68, Loss: 8859.548130580357, Acc: 37.9874213836478\n",
      "Training VaDE... Epoch: 69, Loss: 8833.32798549107\n",
      "Testing VaDE... Epoch: 69, Loss: 8800.64662388393, Acc: 37.10691823899371\n",
      "Training VaDE... Epoch: 70, Loss: 8772.373325892857\n",
      "Testing VaDE... Epoch: 70, Loss: 8722.0322265625, Acc: 36.477987421383645\n",
      "Training VaDE... Epoch: 71, Loss: 8739.1005859375\n",
      "Testing VaDE... Epoch: 71, Loss: 8735.564174107143, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 72, Loss: 8738.771205357143\n",
      "Testing VaDE... Epoch: 72, Loss: 8702.98842075893, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 73, Loss: 8714.486746651786\n",
      "Testing VaDE... Epoch: 73, Loss: 8731.811941964286, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 74, Loss: 8694.17857142857\n",
      "Testing VaDE... Epoch: 74, Loss: 8681.237025669643, Acc: 36.477987421383645\n",
      "Training VaDE... Epoch: 75, Loss: 8705.255440848214\n",
      "Testing VaDE... Epoch: 75, Loss: 8666.23423549107, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 76, Loss: 8700.555106026786\n",
      "Testing VaDE... Epoch: 76, Loss: 8636.97739955357, Acc: 37.35849056603773\n",
      "Training VaDE... Epoch: 77, Loss: 8592.252650669643\n",
      "Testing VaDE... Epoch: 77, Loss: 8603.4482421875, Acc: 36.9811320754717\n",
      "Training VaDE... Epoch: 78, Loss: 8619.96303013393\n",
      "Testing VaDE... Epoch: 78, Loss: 8631.3583984375, Acc: 38.61635220125786\n",
      "Training VaDE... Epoch: 79, Loss: 8588.674246651786\n",
      "Testing VaDE... Epoch: 79, Loss: 8549.189871651786, Acc: 37.10691823899371\n",
      "Training VaDE... Epoch: 80, Loss: 8602.159598214286\n",
      "Testing VaDE... Epoch: 80, Loss: 8555.1875, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 81, Loss: 8552.918526785714\n",
      "Testing VaDE... Epoch: 81, Loss: 8554.005859375, Acc: 37.484276729559745\n",
      "Training VaDE... Epoch: 82, Loss: 8520.900948660714\n",
      "Testing VaDE... Epoch: 82, Loss: 8550.134486607143, Acc: 35.9748427672956\n",
      "Training VaDE... Epoch: 83, Loss: 8512.4111328125\n",
      "Testing VaDE... Epoch: 83, Loss: 8535.931361607143, Acc: 37.10691823899371\n",
      "Training VaDE... Epoch: 84, Loss: 8539.52455357143\n",
      "Testing VaDE... Epoch: 84, Loss: 8511.910435267857, Acc: 36.72955974842767\n",
      "Training VaDE... Epoch: 85, Loss: 8469.191824776786\n",
      "Testing VaDE... Epoch: 85, Loss: 8491.841796875, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 86, Loss: 8500.514578683036\n",
      "Testing VaDE... Epoch: 86, Loss: 8486.972377232143, Acc: 35.471698113207545\n",
      "Training VaDE... Epoch: 87, Loss: 8454.747349330357\n",
      "Testing VaDE... Epoch: 87, Loss: 8416.25355747768, Acc: 36.72955974842767\n",
      "Training VaDE... Epoch: 88, Loss: 8470.923828125\n",
      "Testing VaDE... Epoch: 88, Loss: 8416.21763392857, Acc: 36.9811320754717\n",
      "Training VaDE... Epoch: 89, Loss: 8426.489815848214\n",
      "Testing VaDE... Epoch: 89, Loss: 8427.275111607143, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 90, Loss: 8380.56089564732\n",
      "Testing VaDE... Epoch: 90, Loss: 8411.30580357143, Acc: 35.84905660377358\n",
      "Training VaDE... Epoch: 91, Loss: 8359.033412388393\n",
      "Testing VaDE... Epoch: 91, Loss: 8363.6953125, Acc: 35.9748427672956\n",
      "Training VaDE... Epoch: 92, Loss: 8421.53501674107\n",
      "Testing VaDE... Epoch: 92, Loss: 8417.732700892857, Acc: 37.86163522012579\n",
      "Training VaDE... Epoch: 93, Loss: 8388.346261160714\n",
      "Testing VaDE... Epoch: 93, Loss: 8423.225167410714, Acc: 36.10062893081761\n",
      "Training VaDE... Epoch: 94, Loss: 8316.560267857143\n",
      "Testing VaDE... Epoch: 94, Loss: 8356.0556640625, Acc: 37.9874213836478\n",
      "Training VaDE... Epoch: 95, Loss: 8318.97816685268\n",
      "Testing VaDE... Epoch: 95, Loss: 8347.9951171875, Acc: 37.23270440251572\n",
      "Training VaDE... Epoch: 96, Loss: 8330.62904575893\n",
      "Testing VaDE... Epoch: 96, Loss: 8357.234165736607, Acc: 36.22641509433962\n",
      "Training VaDE... Epoch: 97, Loss: 8337.879464285714\n",
      "Testing VaDE... Epoch: 97, Loss: 8333.99888392857, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 98, Loss: 8364.14683314732\n",
      "Testing VaDE... Epoch: 98, Loss: 8313.825613839286, Acc: 36.35220125786163\n",
      "Training VaDE... Epoch: 99, Loss: 8305.12583705357\n",
      "Testing VaDE... Epoch: 99, Loss: 8294.987583705357, Acc: 37.10691823899371\n",
      "Training VaDE... Epoch: 100, Loss: 8278.283621651786\n",
      "Testing VaDE... Epoch: 100, Loss: 8313.919921875, Acc: 35.84905660377358\n",
      "Training VaDE... Epoch: 101, Loss: 8284.802804129464\n",
      "Testing VaDE... Epoch: 101, Loss: 8283.52866908482, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 102, Loss: 8222.58168247768\n",
      "Testing VaDE... Epoch: 102, Loss: 8251.912806919643, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 103, Loss: 8273.560965401786\n",
      "Testing VaDE... Epoch: 103, Loss: 8264.75258091518, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 104, Loss: 8285.98744419643\n",
      "Testing VaDE... Epoch: 104, Loss: 8233.633998325893, Acc: 36.10062893081761\n",
      "Training VaDE... Epoch: 105, Loss: 8231.552525111607\n",
      "Testing VaDE... Epoch: 105, Loss: 8263.608607700893, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 106, Loss: 8237.8447265625\n",
      "Testing VaDE... Epoch: 106, Loss: 8180.346261160715, Acc: 36.35220125786163\n",
      "Training VaDE... Epoch: 107, Loss: 8227.61363002232\n",
      "Testing VaDE... Epoch: 107, Loss: 8238.86572265625, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 108, Loss: 8255.791085379464\n",
      "Testing VaDE... Epoch: 108, Loss: 8230.78076171875, Acc: 34.84276729559748\n",
      "Training VaDE... Epoch: 109, Loss: 8212.647530691964\n",
      "Testing VaDE... Epoch: 109, Loss: 8217.88999720982, Acc: 36.35220125786163\n",
      "Training VaDE... Epoch: 110, Loss: 8226.62074497768\n",
      "Testing VaDE... Epoch: 110, Loss: 8213.812569754464, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 111, Loss: 8179.6064453125\n",
      "Testing VaDE... Epoch: 111, Loss: 8222.340122767857, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 112, Loss: 8201.37290736607\n",
      "Testing VaDE... Epoch: 112, Loss: 8182.59716796875, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 113, Loss: 8129.495047433035\n",
      "Testing VaDE... Epoch: 113, Loss: 8224.52671595982, Acc: 36.22641509433962\n",
      "Training VaDE... Epoch: 114, Loss: 8172.821637834822\n",
      "Testing VaDE... Epoch: 114, Loss: 8197.32080078125, Acc: 35.72327044025157\n",
      "Training VaDE... Epoch: 115, Loss: 8125.276088169643\n",
      "Testing VaDE... Epoch: 115, Loss: 8156.829938616072, Acc: 35.34591194968553\n",
      "Training VaDE... Epoch: 116, Loss: 8190.083984375\n",
      "Testing VaDE... Epoch: 116, Loss: 8122.501604352678, Acc: 35.471698113207545\n",
      "Training VaDE... Epoch: 117, Loss: 8179.06689453125\n",
      "Testing VaDE... Epoch: 117, Loss: 8179.272391183035, Acc: 35.9748427672956\n",
      "Training VaDE... Epoch: 118, Loss: 8174.039899553572\n",
      "Testing VaDE... Epoch: 118, Loss: 8157.341029575893, Acc: 35.34591194968553\n",
      "Training VaDE... Epoch: 119, Loss: 8140.738420758928\n",
      "Testing VaDE... Epoch: 119, Loss: 8153.6533203125, Acc: 35.471698113207545\n",
      "Training VaDE... Epoch: 120, Loss: 8120.319126674107\n",
      "Testing VaDE... Epoch: 120, Loss: 8110.964285714285, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 121, Loss: 8094.6875\n",
      "Testing VaDE... Epoch: 121, Loss: 8135.808523995535, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 122, Loss: 8132.503976004465\n",
      "Testing VaDE... Epoch: 122, Loss: 8084.679827008928, Acc: 34.9685534591195\n",
      "Training VaDE... Epoch: 123, Loss: 8085.204799107143\n",
      "Testing VaDE... Epoch: 123, Loss: 8072.591936383928, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 124, Loss: 8109.64208984375\n",
      "Testing VaDE... Epoch: 124, Loss: 8082.528738839285, Acc: 35.9748427672956\n",
      "Training VaDE... Epoch: 125, Loss: 8057.766671316965\n",
      "Testing VaDE... Epoch: 125, Loss: 8083.92431640625, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 126, Loss: 8101.768136160715\n",
      "Testing VaDE... Epoch: 126, Loss: 8044.925432477678, Acc: 34.71698113207547\n",
      "Training VaDE... Epoch: 127, Loss: 8107.915318080357\n",
      "Testing VaDE... Epoch: 127, Loss: 8092.371861049107, Acc: 35.9748427672956\n",
      "Training VaDE... Epoch: 128, Loss: 8063.214634486607\n",
      "Testing VaDE... Epoch: 128, Loss: 8111.1552734375, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 129, Loss: 8080.796316964285\n",
      "Testing VaDE... Epoch: 129, Loss: 8028.020298549107, Acc: 35.34591194968553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VaDE... Epoch: 130, Loss: 8057.105538504465\n",
      "Testing VaDE... Epoch: 130, Loss: 8093.705636160715, Acc: 34.71698113207547\n",
      "Training VaDE... Epoch: 131, Loss: 8053.224051339285\n",
      "Testing VaDE... Epoch: 131, Loss: 8039.231515066965, Acc: 34.21383647798742\n",
      "Training VaDE... Epoch: 132, Loss: 8049.927594866072\n",
      "Testing VaDE... Epoch: 132, Loss: 8051.276785714285, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 133, Loss: 8076.154994419643\n",
      "Testing VaDE... Epoch: 133, Loss: 8040.512276785715, Acc: 34.71698113207547\n",
      "Training VaDE... Epoch: 134, Loss: 7985.143484933035\n",
      "Testing VaDE... Epoch: 134, Loss: 8052.878138950893, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 135, Loss: 8030.164132254465\n",
      "Testing VaDE... Epoch: 135, Loss: 8038.315848214285, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 136, Loss: 8066.943289620535\n",
      "Testing VaDE... Epoch: 136, Loss: 8038.985212053572, Acc: 34.59119496855346\n",
      "Training VaDE... Epoch: 137, Loss: 8070.670828683035\n",
      "Testing VaDE... Epoch: 137, Loss: 8049.733468191965, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 138, Loss: 7972.273995535715\n",
      "Testing VaDE... Epoch: 138, Loss: 8021.319545200893, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 139, Loss: 7992.101771763393\n",
      "Testing VaDE... Epoch: 139, Loss: 8034.074567522322, Acc: 34.088050314465406\n",
      "Training VaDE... Epoch: 140, Loss: 7994.377022879465\n",
      "Testing VaDE... Epoch: 140, Loss: 8036.076729910715, Acc: 35.59748427672956\n",
      "Training VaDE... Epoch: 141, Loss: 8011.617047991072\n",
      "Testing VaDE... Epoch: 141, Loss: 8018.336146763393, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 142, Loss: 8007.07470703125\n",
      "Testing VaDE... Epoch: 142, Loss: 8018.494698660715, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 143, Loss: 8017.588448660715\n",
      "Testing VaDE... Epoch: 143, Loss: 8032.695940290178, Acc: 35.22012578616352\n",
      "Training VaDE... Epoch: 144, Loss: 8018.415387834822\n",
      "Testing VaDE... Epoch: 144, Loss: 7955.923967633928, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 145, Loss: 8014.413922991072\n",
      "Testing VaDE... Epoch: 145, Loss: 8060.447823660715, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 146, Loss: 7995.096400669643\n",
      "Testing VaDE... Epoch: 146, Loss: 7997.811732700893, Acc: 33.58490566037736\n",
      "Training VaDE... Epoch: 147, Loss: 7967.050990513393\n",
      "Testing VaDE... Epoch: 147, Loss: 7984.310756138393, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 148, Loss: 7996.419363839285\n",
      "Testing VaDE... Epoch: 148, Loss: 7980.445452008928, Acc: 33.20754716981132\n",
      "Training VaDE... Epoch: 149, Loss: 7958.398228236607\n",
      "Testing VaDE... Epoch: 149, Loss: 7980.012765066965, Acc: 33.33333333333333\n",
      "Training VaDE... Epoch: 150, Loss: 7983.66357421875\n",
      "Testing VaDE... Epoch: 150, Loss: 7979.790318080357, Acc: 36.35220125786163\n",
      "Training VaDE... Epoch: 151, Loss: 7987.893205915178\n",
      "Testing VaDE... Epoch: 151, Loss: 7921.483189174107, Acc: 33.58490566037736\n",
      "Training VaDE... Epoch: 152, Loss: 8008.445242745535\n",
      "Testing VaDE... Epoch: 152, Loss: 7976.812848772322, Acc: 33.71069182389937\n",
      "Training VaDE... Epoch: 153, Loss: 7930.46533203125\n",
      "Testing VaDE... Epoch: 153, Loss: 7963.457589285715, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 154, Loss: 7966.010672433035\n",
      "Testing VaDE... Epoch: 154, Loss: 7905.463727678572, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 155, Loss: 7984.062151227678\n",
      "Testing VaDE... Epoch: 155, Loss: 7924.910853794643, Acc: 34.71698113207547\n",
      "Training VaDE... Epoch: 156, Loss: 7953.899344308035\n",
      "Testing VaDE... Epoch: 156, Loss: 7952.4853515625, Acc: 34.088050314465406\n",
      "Training VaDE... Epoch: 157, Loss: 7948.113351004465\n",
      "Testing VaDE... Epoch: 157, Loss: 7892.0146484375, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 158, Loss: 7939.892647879465\n",
      "Testing VaDE... Epoch: 158, Loss: 7966.969377790178, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 159, Loss: 7947.594447544643\n",
      "Testing VaDE... Epoch: 159, Loss: 7969.461146763393, Acc: 32.20125786163522\n",
      "Training VaDE... Epoch: 160, Loss: 7973.427385602678\n",
      "Testing VaDE... Epoch: 160, Loss: 7888.932686941965, Acc: 36.35220125786163\n",
      "Training VaDE... Epoch: 161, Loss: 7956.978236607143\n",
      "Testing VaDE... Epoch: 161, Loss: 7916.16357421875, Acc: 33.58490566037736\n",
      "Training VaDE... Epoch: 162, Loss: 7955.229910714285\n",
      "Testing VaDE... Epoch: 162, Loss: 7934.644461495535, Acc: 34.9685534591195\n",
      "Training VaDE... Epoch: 163, Loss: 7935.760881696428\n",
      "Testing VaDE... Epoch: 163, Loss: 7948.687430245535, Acc: 32.83018867924528\n",
      "Training VaDE... Epoch: 164, Loss: 7933.425641741072\n",
      "Testing VaDE... Epoch: 164, Loss: 7982.907784598215, Acc: 35.094339622641506\n",
      "Training VaDE... Epoch: 165, Loss: 7970.381417410715\n",
      "Testing VaDE... Epoch: 165, Loss: 7940.108049665178, Acc: 32.20125786163522\n",
      "Training VaDE... Epoch: 166, Loss: 7937.523088727678\n",
      "Testing VaDE... Epoch: 166, Loss: 7896.723563058035, Acc: 33.71069182389937\n",
      "Training VaDE... Epoch: 167, Loss: 7949.153878348215\n",
      "Testing VaDE... Epoch: 167, Loss: 7956.145368303572, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 168, Loss: 7966.402483258928\n",
      "Testing VaDE... Epoch: 168, Loss: 7907.48193359375, Acc: 33.20754716981132\n",
      "Training VaDE... Epoch: 169, Loss: 7913.865931919643\n",
      "Testing VaDE... Epoch: 169, Loss: 7897.077706473215, Acc: 33.83647798742138\n",
      "Training VaDE... Epoch: 170, Loss: 7910.259765625\n",
      "Testing VaDE... Epoch: 170, Loss: 7922.11669921875, Acc: 33.71069182389937\n",
      "Training VaDE... Epoch: 171, Loss: 7915.070731026785\n",
      "Testing VaDE... Epoch: 171, Loss: 7924.906110491072, Acc: 33.83647798742138\n",
      "Training VaDE... Epoch: 172, Loss: 7914.97314453125\n",
      "Testing VaDE... Epoch: 172, Loss: 7921.750558035715, Acc: 34.84276729559748\n",
      "Training VaDE... Epoch: 173, Loss: 7912.905622209822\n",
      "Testing VaDE... Epoch: 173, Loss: 7939.399204799107, Acc: 34.9685534591195\n",
      "Training VaDE... Epoch: 174, Loss: 7908.840471540178\n",
      "Testing VaDE... Epoch: 174, Loss: 7865.108468191965, Acc: 34.59119496855346\n",
      "Training VaDE... Epoch: 175, Loss: 7879.226981026785\n",
      "Testing VaDE... Epoch: 175, Loss: 7891.667131696428, Acc: 32.20125786163522\n",
      "Training VaDE... Epoch: 176, Loss: 7902.270717075893\n",
      "Testing VaDE... Epoch: 176, Loss: 7921.759626116072, Acc: 34.088050314465406\n",
      "Training VaDE... Epoch: 177, Loss: 7902.612583705357\n",
      "Testing VaDE... Epoch: 177, Loss: 7910.563755580357, Acc: 33.459119496855344\n",
      "Training VaDE... Epoch: 178, Loss: 7871.034946986607\n",
      "Testing VaDE... Epoch: 178, Loss: 7866.320103236607, Acc: 34.33962264150943\n",
      "Training VaDE... Epoch: 179, Loss: 7880.786481584822\n",
      "Testing VaDE... Epoch: 179, Loss: 7857.12451171875, Acc: 33.58490566037736\n",
      "Training VaDE... Epoch: 180, Loss: 7885.7373046875\n",
      "Testing VaDE... Epoch: 180, Loss: 7852.028041294643, Acc: 32.9559748427673\n",
      "Training VaDE... Epoch: 181, Loss: 7891.833565848215\n",
      "Testing VaDE... Epoch: 181, Loss: 7874.090262276785, Acc: 33.71069182389937\n",
      "Training VaDE... Epoch: 182, Loss: 7823.082798549107\n",
      "Testing VaDE... Epoch: 182, Loss: 7878.329520089285, Acc: 34.71698113207547\n",
      "Training VaDE... Epoch: 183, Loss: 7894.075404575893\n",
      "Testing VaDE... Epoch: 183, Loss: 7892.451311383928, Acc: 33.33333333333333\n",
      "Training VaDE... Epoch: 184, Loss: 7831.164969308035\n",
      "Testing VaDE... Epoch: 184, Loss: 7858.30029296875, Acc: 33.33333333333333\n",
      "Training VaDE... Epoch: 185, Loss: 7871.861537388393\n",
      "Testing VaDE... Epoch: 185, Loss: 7867.549386160715, Acc: 33.459119496855344\n",
      "Training VaDE... Epoch: 186, Loss: 7870.948869977678\n",
      "Testing VaDE... Epoch: 186, Loss: 7849.045549665178, Acc: 33.459119496855344\n",
      "Training VaDE... Epoch: 187, Loss: 7864.408970424107\n",
      "Testing VaDE... Epoch: 187, Loss: 7858.554408482143, Acc: 32.45283018867924\n",
      "Training VaDE... Epoch: 188, Loss: 7841.189383370535\n",
      "Testing VaDE... Epoch: 188, Loss: 7911.866420200893, Acc: 32.83018867924528\n",
      "Training VaDE... Epoch: 189, Loss: 7856.459263392857\n",
      "Testing VaDE... Epoch: 189, Loss: 7874.573172433035, Acc: 32.45283018867924\n",
      "Training VaDE... Epoch: 190, Loss: 7877.603934151785\n",
      "Testing VaDE... Epoch: 190, Loss: 7923.535016741072, Acc: 33.20754716981132\n",
      "Training VaDE... Epoch: 191, Loss: 7835.249651227678\n",
      "Testing VaDE... Epoch: 191, Loss: 7897.607491629465, Acc: 34.465408805031444\n",
      "Training VaDE... Epoch: 192, Loss: 7889.288504464285\n",
      "Testing VaDE... Epoch: 192, Loss: 7825.306012834822, Acc: 33.459119496855344\n",
      "Training VaDE... Epoch: 193, Loss: 7877.488141741072\n",
      "Testing VaDE... Epoch: 193, Loss: 7844.688058035715, Acc: 34.088050314465406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VaDE... Epoch: 194, Loss: 7856.3837890625\n",
      "Testing VaDE... Epoch: 194, Loss: 7864.360491071428, Acc: 32.83018867924528\n",
      "Training VaDE... Epoch: 195, Loss: 7865.822684151785\n",
      "Testing VaDE... Epoch: 195, Loss: 7867.193708147322, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 196, Loss: 7869.461774553572\n",
      "Testing VaDE... Epoch: 196, Loss: 7822.260602678572, Acc: 33.9622641509434\n",
      "Training VaDE... Epoch: 197, Loss: 7883.801339285715\n",
      "Testing VaDE... Epoch: 197, Loss: 7833.176339285715, Acc: 32.83018867924528\n",
      "Training VaDE... Epoch: 198, Loss: 7851.275809151785\n",
      "Testing VaDE... Epoch: 198, Loss: 7879.449079241072, Acc: 34.59119496855346\n",
      "Training VaDE... Epoch: 199, Loss: 7891.806780133928\n",
      "Testing VaDE... Epoch: 199, Loss: 7808.944475446428, Acc: 33.83647798742138\n"
     ]
    }
   ],
   "source": [
    "vade.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "ftr_ext = vade.feature_extractor\n",
    "model = vade.VaDE\n",
    "z, y = get_latent_space(dataloader, z_dim, model, device, ftr_ext)\n",
    "z, y = z.cpu(), y.cpu()\n",
    "z_embedded = TSNE(n_components=2).fit_transform(z.detach().numpy())\n",
    "plot_tsne(z_embedded, y, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
