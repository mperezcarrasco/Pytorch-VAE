import math
import torch
import numpy as np
from torch import optim
import torch.nn.functional as F
from sklearn.mixture import GaussianMixture
from sklearn.utils.linear_assignment_ import linear_assignment


def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Linear") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)

class TrainerVaDE:
    """This is the trainer for the Variational Deep Embedding (VaDE).
    """
    def __init__(self, args, device, dataloader_train, dataloader_test, n_classes):
        if args.dataset == 'mnist':
            from models import Autoencoder, VaDE
            self.autoencoder = Autoencoder().to(device)
            self.autoencoder.apply(weights_init_normal)
            self.VaDE = VaDE().to(device)
        elif args.dataset == 'webcam':
            from models_office import Autoencoder, VaDE, feature_extractor
            self.autoencoder = Autoencoder().to(device)
            checkpoint = torch.load('weights/imagenet_params.pth.tar',
                                    map_location=device)
            self.autoencoder.load_state_dict(checkpoint['state_dict'], strict=False)
            checkpoint = torch.load('weights/feature_extractor_params.pth.tar',
                                     map_location=device)
            self.feature_extractor = feature_extractor().to(device)
            self.feature_extractor.load_state_dict(checkpoint['state_dict'])
            self.freeze_extractor()
            self.VaDE = VaDE().to(device)

        self.dataloader_train = dataloader_train
        self.dataloader_test = dataloader_test
        self.device = device
        self.args = args
        self.n_classes = n_classes


    def pretrain(self):
        """Here we train an stacked autoencoder which will be used as the initialization for the VaDE. 
        This initialization is usefull because reconstruction in VAEs would be weak at the begining
        and the models are likely to get stuck in local minima.
        """
        self.loss_ae = []
        optimizer = optim.Adam(self.autoencoder.parameters(), lr=self.args.lr_ae)
        self.autoencoder.train()
        print('Training the autoencoder...')
        for epoch in range(1500):
            total_loss = 0
            for x, _ in self.dataloader_train:
                optimizer.zero_grad()
                x = x.to(self.device)
                if self.args.dataset == 'webcam':
                    x = self.feature_extractor(x)
                    x = x.detach()
                x_hat = self.autoencoder(x)
                loss = F.mse_loss(x_hat, x, reduction='mean') #reconstruction error
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            self.loss_ae.append(total_loss/len(self.dataloader_train))
            print('Training Autoencoder... Epoch: {}, Loss: {}'.format(epoch, total_loss/len(self.dataloader_train)))
        self.train_GMM() #training a GMM for initialize the VaDE
        self.save_weights_for_VaDE() #saving weights for the VaDE
        self.save_weights_ae()

        
    def train_GMM(self):
        """It is possible to fit a Gaussian Mixture Model (GMM) using the latent space 
        generated by the stacked autoencoder. This way, we generate an initialization for 
        the priors (pi, mu, var) of the VaDE model.
        """
        print('Fiting Gaussian Mixture Model...')
        x = torch.cat([data[0] for data in self.dataloader_train]).to(self.device) #all x samples.
        if self.args.dataset == 'webcam':
            x = self.feature_extractor(x)
            x = x.detach()
        z = self.autoencoder.encode(x)
        self.gmm = GaussianMixture(n_components=self.n_classes, covariance_type='diag')
        self.gmm.fit(z.cpu().detach().numpy())


    def save_weights_for_VaDE(self):
        """Saving the pretrained weights for the encoder, decoder, pi, mu, var.
        """
        print('Saving weights.')
        state_dict = self.autoencoder.state_dict()

        self.VaDE.load_state_dict(state_dict, strict=False)
        self.VaDE.pi_prior.data = torch.from_numpy(self.gmm.weights_).float().to(self.device)
        self.VaDE.mu_prior.data = torch.from_numpy(self.gmm.means_).float().to(self.device)
        self.VaDE.log_var_prior.data = torch.log(torch.from_numpy(self.gmm.covariances_)).float().to(self.device)
        torch.save(self.VaDE.state_dict(), 'weights/pretrained_parameters_{}.pth'.format(self.args.dataset))  

    def save_weights_ae(self):
        """Saving the pretrained weights for the encoder, decoder, pi, mu, var.
        """
        print('Saving weights.')
        state = {'state_dict': self.autoencoder.state_dict()}
        torch.save(state, 'weights/autoencoder_parameters_{}.pth.tar'.format(self.args.dataset)) 

    def save_weights_vade(self):
        """Saving the pretrained weights for the encoder, decoder, pi, mu, var.
        """
        print('Saving weights.')
        state = {'state_dict': self.autoencoder.state_dict()}
        torch.save(state, 'weights/vade_parameters_{}.pth.tar'.format(self.args.dataset)) 
        

    def train(self):
        """
        """
        if self.args.pretrain==True:
            self.VaDE.load_state_dict(torch.load('weights/pretrained_parameters_{}.pth'.format(self.args.dataset),
                                                 map_location=self.device))
        else:
            self.VaDE.apply(weights_init_normal)
        self.optimizer = optim.Adam(self.VaDE.parameters(), lr=self.args.lr)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(
                    self.optimizer, step_size=10, gamma=0.9)
        print('Training VaDE...')
        self.rec = []
        self.dkl_c = []
        self.dkl_z = []
        self.rec_t = []
        self.dkl_c_t = []
        self.dkl_z_t = []
        self.acc_t = []
        self.test_VaDE(-1)
        for epoch in range(self.args.epochs):
            self.train_VaDE(epoch)
            self.test_VaDE(epoch)
            lr_scheduler.step()
        self.save_weights_vade()


    def train_VaDE(self, epoch):
        self.VaDE.train()
        total_loss = 0
        total_rec = 0
        total_dkl_c = 0
        total_dkl_z = 0

        for x, _ in self.dataloader_train:
            self.optimizer.zero_grad()
            x = x.to(self.device)
            if self.args.dataset == 'webcam':
                x = self.feature_extractor(x)
                x = x.detach()
            x_hat, mu, log_var, z = self.VaDE(x)
            loss, reconst, dkl_z, dkl_c = self.compute_loss(x, x_hat, mu, log_var, z)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
            total_rec += reconst.item()
            total_dkl_c += dkl_c.item()
            total_dkl_z += dkl_z.item()
        self.rec.append(total_rec/len(self.dataloader_train))
        self.dkl_c.append(total_dkl_c/len(self.dataloader_train))
        self.dkl_z.append(total_dkl_z/len(self.dataloader_train))
        print('Training VaDE... Epoch: {}, Loss: {}'.format(epoch, 
              total_loss/len(self.dataloader_train)))


    def test_VaDE(self, epoch):
        self.VaDE.eval()
        with torch.no_grad():
            total_loss = 0
            total_rec = 0
            total_dkl_c = 0
            total_dkl_z = 0
            y_true, y_pred = [], []
            for x, true in self.dataloader_test:
                x = x.to(self.device)
                if self.args.dataset == 'webcam':
                    x = self.feature_extractor(x)
                    x = x.detach()
                x_hat, mu, log_var, z = self.VaDE(x)
                gamma = self.compute_gamma(z, self.VaDE.pi_prior)
                pred = torch.argmax(gamma, dim=1)
                loss, reconst, dkl_z, dkl_c = self.compute_loss(x, x_hat, mu, log_var, z)
                total_loss += loss.item()
                total_rec += reconst.item()
                total_dkl_c += dkl_c.item()
                total_dkl_z += dkl_z.item()
                y_true.extend(true.numpy())
                y_pred.extend(pred.cpu().detach().numpy())
            self.rec_t.append(total_rec/len(self.dataloader_test))
            self.dkl_c_t.append(total_dkl_c/len(self.dataloader_test))
            self.dkl_z_t.append(total_dkl_z/len(self.dataloader_test))
            acc = self.cluster_acc(np.array(y_true), np.array(y_pred))
            self.acc_t.append(acc[0])
            print('Testing VaDE... Epoch: {}, Loss: {}, Acc: {}'.format(epoch, 
                   total_loss/len(self.dataloader_test), acc[0]))


    def compute_loss(self, x, x_hat, mu, log_var, z):
        p_c = self.VaDE.pi_prior
        gamma = self.compute_gamma(z, p_c)

        log_p_x_given_z = F.mse_loss(x_hat, x, reduction='sum')
        h = log_var.exp().unsqueeze(1) + (mu.unsqueeze(1) - self.VaDE.mu_prior).pow(2)
        h = torch.sum(self.VaDE.log_var_prior + h / self.VaDE.log_var_prior.exp(), dim=2)
        log_p_z_given_c = 0.5 * torch.sum(gamma * h)
        log_p_c = torch.sum(gamma * torch.log(p_c + 1e-15))
        log_q_c_given_x = torch.sum(gamma * torch.log(gamma + 1e-9))
        log_q_z_given_x = 0.5 * torch.sum(1 + log_var)
        
        reconst = log_p_x_given_z/x.size(0)
        dkl_z = (log_p_z_given_c - log_q_z_given_x)/x.size(0) * 100
        dkl_c = (log_q_c_given_x - log_p_c)/x.size(0)
        loss = reconst + dkl_z + dkl_c
        
        return loss, reconst, dkl_z, dkl_c

    def compute_gamma(self, z, p_c):
        h = (z.unsqueeze(1) - self.VaDE.mu_prior).pow(2) / self.VaDE.log_var_prior.exp()
        h += self.VaDE.log_var_prior
        h += torch.Tensor([np.log(np.pi*2)]).to(self.device)
        p_z_c = torch.exp(torch.log(p_c + 1e-15).unsqueeze(0) - 0.5 * torch.sum(h, dim=2)) + 1e-20
        gamma = p_z_c / torch.sum(p_z_c, dim=1, keepdim=True)
        return gamma

    def cluster_acc(self, real, pred):
        D = max(pred.max(), real.max())+1
        w = np.zeros((D,D), dtype=np.int64)
        for i in range(pred.size):
            w[pred[i], real[i]] += 1
        ind = linear_assignment(w.max() - w)
        return sum([w[i,j] for i,j in ind])*1.0/pred.size*100, w

    def freeze_extractor(self):
        for _, param in self.feature_extractor.named_parameters():
            param.requires_grad = False
        self.feature_extractor.eval()
